{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Landsat Vegetation Phenology\n",
    "\n",
    "<hr>\n",
    "\n",
    "# Notebook Summary\n",
    "\n",
    "This notebook calculates vegetation phenology changes using Landsat 7 or Landsat 8 data. To detect changes in plant life for Landsat, the algorithm uses either the Normalized Difference Vegetation Index (NDVI) or the Enhanced Vegetation Index (EVI), which are common proxies for vegetation growth and health. The outputs of this notebook can be used to assess differences in agriculture fields over time or space and also allow the assessment of growing states such as planting and harvesting.  \n",
    "<br>\n",
    "There are two output products. The first output product is a time series boxplot of NDVI or EVI with the data potentially binned by week, month, week of year, or month of year. The second output product is a time series lineplot of the mean NDVI or EVI for each year, with the data potentially binned by week or month. This product is useful for comparing years to each other.\n",
    "<br><br>\n",
    "See this website for more information: https://phenology.cr.usgs.gov/ndvi_foundation.php\n",
    "\n",
    "<hr>\n",
    "\n",
    "# Index\n",
    "\n",
    "* [Import Dependencies and Connect to the Data Cube](#import)\n",
    "* [Choose Platforms and Products](#plat_prod)\n",
    "* [Get the Extents of the Cube](#extents)\n",
    "* [Define the Extents of the Analysis](#define_extents)\n",
    "* [Load Data from the Data Cube and Obtain the Vegetation Proxy](#load_data)\n",
    "* [Create Phenology Products](#phenology_products)\n",
    "    * [Plot the Vegetation Index Over Time in a Box-and-Whisker Plot](#phenology_plot_1)\n",
    "    * [Plot the Vegetation Index Over Time for Each Year](#phenology_plot_2)\n",
    "* [Export Curve Fits to a CSV File](#export)\n",
    "* [Show TIMESAT Stats](#timesat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span id=\"import\">Import Dependencies and Connect to the Data Cube [&#9652;](#top)</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T14:41:20.672526Z",
     "iopub.status.busy": "2020-09-29T14:41:20.672096Z",
     "iopub.status.idle": "2020-09-29T14:41:22.114243Z",
     "shell.execute_reply": "2020-09-29T14:41:22.113734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Enable importing of utilities.\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.environ.get('NOTEBOOK_ROOT'))\n",
    "\n",
    "# Supress Warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np  \n",
    "import xarray as xr  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "\n",
    "# Load Data Cube Configuration\n",
    "import datacube\n",
    "import utils.data_cube_utilities.data_access_api as dc_api  \n",
    "api = dc_api.DataAccessApi()\n",
    "dc = api.dc\n",
    "\n",
    "from datacube.utils.aws import configure_s3_access\n",
    "configure_s3_access(requester_pays=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span id=\"plat_prod\">Choose Platforms and Products [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List available products for each platform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T14:41:22.118395Z",
     "iopub.status.busy": "2020-09-29T14:41:22.117897Z",
     "iopub.status.idle": "2020-09-29T14:41:22.150515Z",
     "shell.execute_reply": "2020-09-29T14:41:22.150937Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get available products\n",
    "products_info = dc.list_products()\n",
    "\n",
    "# List Landsat 7 products\n",
    "print(\"Landsat 7 Products:\")\n",
    "products_info[[\"platform\", \"name\"]][products_info.platform == \"LANDSAT_7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T14:41:22.158172Z",
     "iopub.status.busy": "2020-09-29T14:41:22.157613Z",
     "iopub.status.idle": "2020-09-29T14:41:22.160489Z",
     "shell.execute_reply": "2020-09-29T14:41:22.160907Z"
    }
   },
   "outputs": [],
   "source": [
    "# List Landsat 8 products\n",
    "print(\"Landsat 8 Products:\")\n",
    "products_info[[\"platform\", \"name\"]][products_info.platform == \"LANDSAT_8\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose products**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\";><b>CHANGE INPUTS BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T14:41:22.164918Z",
     "iopub.status.busy": "2020-09-29T14:41:22.164495Z",
     "iopub.status.idle": "2020-09-29T14:41:22.166589Z",
     "shell.execute_reply": "2020-09-29T14:41:22.166162Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select Products and Platforms\n",
    "# It is possible to select multiple datasets (L7, L8)\n",
    "# True = SELECT\n",
    "# False = DO NOT SELECT\n",
    "\n",
    "use_Landsat7 = False\n",
    "use_Landsat8 = True\n",
    "platforms, products, collections, levels = [], [], [], []\n",
    "if use_Landsat7:\n",
    "    platforms.append('LANDSAT_7')\n",
    "    products.append('ls7_usgs_sr_scene')\n",
    "    collections.append('c1')\n",
    "    levels.append('l2')\n",
    "if use_Landsat8:\n",
    "    platforms.append('LANDSAT_8')\n",
    "    products.append('ls8_usgs_sr_scene')\n",
    "    collections.append('c1')\n",
    "    levels.append('l2')\n",
    "\n",
    "# Select a vegetation proxy to use to track changes in vegetation.\n",
    "# Any one of ['NDVI', 'EVI'] may be chosen, \n",
    "# with 'NDVI' and 'EVI' calculating the spectral index of the same name.\n",
    "veg_proxy = 'EVI' # 'NDVI' is recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span id=\"extents\">Get the Extents of the Cube [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T14:41:22.171758Z",
     "iopub.status.busy": "2020-09-29T14:41:22.171317Z",
     "iopub.status.idle": "2020-09-29T14:41:26.392230Z",
     "shell.execute_reply": "2020-09-29T14:41:26.391783Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.dc_load import get_overlapping_area\n",
    "from utils.data_cube_utilities.dc_time import dt_to_str\n",
    "\n",
    "full_lat, full_lon, min_max_dates = get_overlapping_area(api, platforms, products)\n",
    "\n",
    "# Print the extents of each product.\n",
    "str_min_max_dates = np.vectorize(dt_to_str)(min_max_dates)\n",
    "for i, (platform, product) in enumerate(zip(platforms, products)):\n",
    "    print(\"For platform {} and product {}:\".format(platform, product))\n",
    "    print(\"Time Extents:\", str_min_max_dates[i])\n",
    "    print()\n",
    "\n",
    "# Print the extents of the combined data.\n",
    "min_start_date_mutual = np.max(min_max_dates[:,0])\n",
    "max_end_date_mutual = np.min(min_max_dates[:,1])\n",
    "print(\"Overlapping Extents:\")\n",
    "print(\"Latitude Extents:\", full_lat)\n",
    "print(\"Longitude Extents:\", full_lon)\n",
    "print(\"Time Extents:\", list(map(dt_to_str, (min_start_date_mutual, max_end_date_mutual))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the available area**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T14:41:26.395306Z",
     "iopub.status.busy": "2020-09-29T14:41:26.394895Z",
     "iopub.status.idle": "2020-09-29T14:41:26.571038Z",
     "shell.execute_reply": "2020-09-29T14:41:26.571462Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.dc_display_map import display_map\n",
    "display_map(full_lat, full_lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span id=\"define_extents\">Define the Extents of the Analysis [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\";><b>CHANGE INPUTS BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T14:41:26.579396Z",
     "iopub.status.busy": "2020-09-29T14:41:26.578879Z",
     "iopub.status.idle": "2020-09-29T14:41:26.580613Z",
     "shell.execute_reply": "2020-09-29T14:41:26.581034Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select an analysis region (Lat-Lon) within the extents listed above. \n",
    "# Select a time period (Min-Max) within the extents listed above (Year-Month-Day)\n",
    "\n",
    "# Tanzania Grassland / Cropland\n",
    "# lat = (-4.5074, -4.4860) # North of Swaga Game Reserve\n",
    "# lon = (35.1349, 35.1735) # North of Swaga Game Reserve\n",
    "\n",
    "# Tanzania Grassland / Cropland\n",
    "# lat = (-8.1541, -8.1272) # Southern Cropland \n",
    "# lon = (33.2016, 33.2545) # Southern Cropland\n",
    "\n",
    "# Aviv Coffee Farm, Tanzania (small)\n",
    "# lat = (-10.6999, -10.6959) \n",
    "# lon = (35.2608, 35.2662) \n",
    "\n",
    "# Aviv Coffee Farm, Tanzania (surrounding)\n",
    "# lat = (-10.855, -10.560)\n",
    "# lon = (35.130, 35.400)\n",
    "\n",
    "# Soybean Fields in Western Kenya (from Kizito)\n",
    "# lat = (-0.801180, -0.483689) # entire region\n",
    "# lon = (34.193792, 34.546329) # entire region\n",
    "\n",
    "# Vietnam\n",
    "# lat = (10.9358, 11.0358)\n",
    "# lon = (107.1899, 107.2899)\n",
    "\n",
    "# Ghana\n",
    "lat = (5.5813, 5.6004)\n",
    "lon = (-0.5398, -0.5203)\n",
    "\n",
    "# Time Period\n",
    "start_date, end_date = dt.datetime(2017,1,1), dt.datetime(2017,12,31)\n",
    "time_extents = (start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the selected area**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T14:41:26.584407Z",
     "iopub.status.busy": "2020-09-29T14:41:26.584002Z",
     "iopub.status.idle": "2020-09-29T14:41:26.592037Z",
     "shell.execute_reply": "2020-09-29T14:41:26.592447Z"
    }
   },
   "outputs": [],
   "source": [
    "display_map(lat, lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span id=\"load_data\">Load Data from the Data Cube and Obtain the Vegetation Proxy [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T14:41:26.600760Z",
     "iopub.status.busy": "2020-09-29T14:41:26.600320Z",
     "iopub.status.idle": "2020-09-29T14:41:27.385440Z",
     "shell.execute_reply": "2020-09-29T14:41:27.384551Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.dc_load import match_dim_sizes\n",
    "from utils.data_cube_utilities.clean_mask import landsat_clean_mask_full\n",
    "#     landsat_qa_clean_mask, landsat_clean_mask_invalid\n",
    "from utils.data_cube_utilities.sort import xarray_sortby_coord\n",
    "from utils.data_cube_utilities.dc_load import is_dataset_empty\n",
    "\n",
    "measurements = []\n",
    "if veg_proxy == 'NDVI':\n",
    "    measurements = ['red', 'nir', 'pixel_qa']\n",
    "elif veg_proxy == 'EVI':\n",
    "    measurements = ['red', 'blue', 'nir', 'pixel_qa']\n",
    "nodata_vals = dc.list_measurements().loc[product]['nodata']\n",
    "datasets, clean_masks = {}, {}\n",
    "matching_abs_res, same_dim_sizes = match_dim_sizes(dc, products, lon, lat)\n",
    "for platform, product, collection, level in zip(platforms, products, collections, levels):\n",
    "    # Load the data.\n",
    "    dataset = dc.load(platform=platform, product=product, lat=lat, lon=lon, \n",
    "                      time=time_extents, measurements=measurements, dask_chunks={'time':1})\n",
    "    if len(dataset.dims) == 0: # The dataset is empty.\n",
    "        continue\n",
    "    # Get the clean mask.\n",
    "#     plt_col_lvl_params = dict(platform=platform, collection=collection, level=level)\n",
    "#     clean_mask = (landsat_qa_clean_mask(dataset, **plt_col_lvl_params) & \n",
    "#                   any([dataset[data_var] != nodata_vals[data_var] for data_var in dataset]) & \n",
    "#                   landsat_clean_mask_invalid(dataset, **plt_col_lvl_params))\n",
    "    clean_mask = landsat_clean_mask_full(dc, dataset, product=product, platform=platform, \n",
    "                                         collection=collection, level=level)\n",
    "    dataset = dataset.drop('pixel_qa')\n",
    "    # If needed, scale the datasets and clean masks to the same size in the x and y dimensions.\n",
    "    if not same_dim_sizes:    \n",
    "        dataset = xr_scale_res(dataset, abs_res=matching_abs_res)\n",
    "        clean_mask = xr_scale_res(clean_mask.astype(np.uint8), \\\n",
    "                                  abs_res=matching_abs_res).astype(np.bool)\n",
    "    dataset = dataset.astype(np.float32).where(clean_mask)\n",
    "    # Collect the dataset and clean mask.\n",
    "    datasets[platform] = dataset\n",
    "    clean_masks[platform] = clean_mask\n",
    "# Combine everything.\n",
    "if len(datasets) > 0:\n",
    "    dataset = xarray_sortby_coord(\n",
    "        xr.concat(list(datasets.values()), dim='time'), coord='time')\n",
    "    clean_mask = xarray_sortby_coord(\n",
    "        xr.concat(list(clean_masks.values()), dim='time'), coord='time')\n",
    "else:\n",
    "    dataset = xr.Dataset()\n",
    "    clean_mask = xr.DataArray(np.empty((0,), dtype=np.bool))\n",
    "del datasets, clean_masks\n",
    "\n",
    "assert not is_dataset_empty(dataset), \"The dataset is empty.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T14:41:27.390855Z",
     "iopub.status.busy": "2020-09-29T14:41:27.390037Z",
     "iopub.status.idle": "2020-09-29T14:41:27.397213Z",
     "shell.execute_reply": "2020-09-29T14:41:27.397654Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.vegetation import NDVI, EVI\n",
    "\n",
    "if veg_proxy == 'NDVI':\n",
    "    dataset[veg_proxy] = NDVI(dataset)\n",
    "if veg_proxy == 'EVI':\n",
    "    dataset[veg_proxy] = EVI(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span id=\"phenology_products\">Create Phenology Products[&#9652;](#top)</span>\n",
    "\n",
    "If no plots appear in the figures below, there is no data available for the region selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span id=\"phenology_plot_1\">Plot the Vegetation Index Over Time in a Box-and-Whisker Plot[&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\";><b>CHANGE INPUTS BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T14:41:27.404025Z",
     "iopub.status.busy": "2020-09-29T14:41:27.403611Z",
     "iopub.status.idle": "2020-09-29T14:41:27.409645Z",
     "shell.execute_reply": "2020-09-29T14:41:27.409186Z"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "from utils.data_cube_utilities import plotter_utils\n",
    "importlib.reload(plotter_utils)\n",
    "from utils.data_cube_utilities.plotter_utils import xarray_time_series_plot\n",
    "\n",
    "# Specify whether to plot a curve fit of the vegetation index along time. Input can be either TRUE or FALSE\n",
    "plot_curve_fit = True\n",
    "assert isinstance(plot_curve_fit, bool), \"The variable 'plot_curve_fit' must be \"\\\n",
    "                                         \"either True or False.\"\n",
    "\n",
    "# Specify the target aggregation type of the curve fit. Input can be either 'mean' or 'median'.\n",
    "curve_fit_target = 'median'\n",
    "assert curve_fit_target in ['mean', 'median'], \"The variable 'curve_fit_target' must be either \"\\\n",
    "                                               \"'mean' or 'median'.\"\n",
    "\n",
    "# The maximum number of data points that appear along time in each plot.\n",
    "# If more than this number of data points need to be plotted, a grid of plots will be created.\n",
    "max_times_per_plot = 40 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\";><b>CHANGE INPUTS BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T14:41:27.420986Z",
     "iopub.status.busy": "2020-09-29T14:41:27.420049Z",
     "iopub.status.idle": "2020-09-29T14:41:27.817329Z",
     "shell.execute_reply": "2020-09-29T14:41:27.817794Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select the binning approach for the vegetation index. Set the 'bin_by' parameter.\n",
    "# None          = do not bin the data\n",
    "# 'week'        = bin the data by week with an extended time axis\n",
    "# 'month'       = bin the data by month with an extended time axis\n",
    "# 'weekofyear'  = bin the data by week and years using a single year time axis\n",
    "# 'monthofyear' = bin the data by month and years using a single year time axis\n",
    "\n",
    "# It is also possible to change some of the plotting features using the code below.\n",
    "\n",
    "bin_by = 'monthofyear'\n",
    "assert bin_by in [None, 'week', 'month', 'weekofyear', 'monthofyear'], \\\n",
    "    \"The variable 'bin_by' can only have one of these values: \"\\\n",
    "    \"[None, 'week', 'month', 'weekofyear', 'monthofyear']\"\n",
    "\n",
    "aggregated_by_str = None\n",
    "if bin_by is None:\n",
    "    plotting_data = dataset\n",
    "elif bin_by == 'week':\n",
    "    plotting_data = dataset.resample(time='1w').mean()\n",
    "    aggregated_by_str = 'Week'\n",
    "elif bin_by == 'month':\n",
    "    plotting_data = dataset.resample(time='1m').mean()\n",
    "    aggregated_by_str = 'Month'\n",
    "elif bin_by == 'weekofyear':\n",
    "    plotting_data = dataset.groupby('time.week').mean(dim=('time'))\n",
    "    aggregated_by_str = 'Week of Year'\n",
    "elif bin_by == 'monthofyear':\n",
    "    plotting_data = dataset.groupby('time.month').mean(dim=('time'))\n",
    "    aggregated_by_str = 'Month of Year'\n",
    "    \n",
    "params = dict(dataset=plotting_data, plot_descs={veg_proxy:{'none':[\n",
    "    {'box':{'boxprops':{'facecolor':'forestgreen'}}}]}})\n",
    "if plot_curve_fit:\n",
    "    params['plot_descs'][veg_proxy][curve_fit_target] = [{'gaussian_filter':{}}]\n",
    "    \n",
    "xarray_time_series_plot(**params, fig_params=dict(figsize=(8,4), dpi=150), \n",
    "                        max_times_per_plot=max_times_per_plot)\n",
    "plt.title('Box-and-Whisker Plot of {1} with a Curvefit of {0} {1}'\n",
    "          .format(curve_fit_target.capitalize(), veg_proxy))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span id=\"phenology_plot_2\">Plot the Vegetation Index Over Time for Each Year[&#9652;](#top)</span>\n",
    "Note that the curve fits here do not show where some times have no data (encoded as NaNs), as is shown in the box-and-whisker plot. Notably, the curve fits interpolate over times with missing data that are not the first or last time (e.g. January or December for monthly binned data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\";><b>CHANGE INPUTS BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T14:41:27.835895Z",
     "iopub.status.busy": "2020-09-29T14:41:27.825145Z",
     "iopub.status.idle": "2020-09-29T14:41:27.995246Z",
     "shell.execute_reply": "2020-09-29T14:41:27.995679Z"
    }
   },
   "outputs": [],
   "source": [
    "years_with_data = []\n",
    "plot_descs = {}\n",
    "daysofyear_per_year = {}\n",
    "plotting_data_years = {}\n",
    "time_dim_name = None\n",
    "for year in range(start_date.year, end_date.year+1):\n",
    "    year_data = dataset.sel(time=slice('{}-01-01'.format(year), '{}-12-31'.format(year)))[veg_proxy]\n",
    "    if len(year_data['time']) == 0: # There is nothing to plot for this year.\n",
    "        print(\"Year {} has no data, so will not be plotted.\".format(year))\n",
    "        continue\n",
    "    years_with_data.append(year)\n",
    "    \n",
    "    spec_ind_dayofyear = year_data.groupby('time.dayofyear').mean()\n",
    "    # Remove times with no data.\n",
    "    daysofyear_per_year[year] = spec_ind_dayofyear[~spec_ind_dayofyear.isnull().all(['latitude', 'longitude'])].dayofyear\n",
    "    \n",
    "# Select the binning approach for the vegetation index. Set the 'bin_by' parameter.\n",
    "# 'weekofyear'  = bin the data by week and years using a single year time axis\n",
    "# 'monthofyear' = bin the data by month and years using a single year time axis\n",
    "\n",
    "    bin_by = 'monthofyear'\n",
    "    \n",
    "    assert bin_by in ['weekofyear', 'monthofyear'], \\\n",
    "        \"The variable 'bin_by' can only have one of these values: \"\\\n",
    "        \"['weekofyear', 'monthofyear']\"\n",
    "    \n",
    "    aggregated_by_str = None\n",
    "    if bin_by == 'weekofyear':\n",
    "        plotting_data_year = year_data.groupby('time.week').mean(dim=('time'))\n",
    "        time_dim_name = 'week'\n",
    "    elif bin_by == 'monthofyear':\n",
    "        plotting_data_year = year_data.groupby('time.month').mean(dim=('time'))\n",
    "        time_dim_name = 'month'\n",
    "\n",
    "    plotting_data_years[year] = plotting_data_year\n",
    "    num_time_pts = len(plotting_data_year[time_dim_name])\n",
    "    \n",
    "    # Select the curve-fit type. \n",
    "    # See the documentation for `xarray_time_series_plot()` regarding the `plot_descs` parameter.\n",
    "    plot_descs[year] = {'mean':[{'gaussian_filter':{}}]}\n",
    "\n",
    "time_dim_name = 'week' if bin_by == 'weekofyear' else 'month' if bin_by == 'monthofyear' else 'time'\n",
    "\n",
    "num_times = 54 if bin_by == 'weekofyear' else 12\n",
    "time_coords_arr = np.arange(1, num_times+1) # In xarray, week and month indices start at 1.\n",
    "time_coords_da = xr.DataArray(time_coords_arr, coords={time_dim_name:time_coords_arr}, \n",
    "                              dims=[time_dim_name], name=time_dim_name)\n",
    "coords = dict(list(plotting_data_years.values())[0].coords)\n",
    "coords[time_dim_name] = time_coords_da \n",
    "plotting_data = xr.Dataset(plotting_data_years, coords=coords)\n",
    "params = dict(dataset=plotting_data, plot_descs=plot_descs)\n",
    "\n",
    "fig, curve_fit_plotting_data = \\\n",
    "    xarray_time_series_plot(**params, fig_params=dict(figsize=(8,4), dpi=150))\n",
    "plt.title('Line Plot of {0} for Each Year'.format(veg_proxy))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span id=\"export\">Export Curve Fits to a CSV File [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T14:41:28.002875Z",
     "iopub.status.busy": "2020-09-29T14:41:28.002353Z",
     "iopub.status.idle": "2020-09-29T14:41:28.008873Z",
     "shell.execute_reply": "2020-09-29T14:41:28.009284Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Convert the data to a `pandas.DataFrame`.\n",
    "dataarrays = []\n",
    "for (year, _, _), dataarray in curve_fit_plotting_data.items():\n",
    "    dataarrays.append(dataarray.rename(year))\n",
    "curve_fit_df = xr.merge(dataarrays).to_dataframe()\n",
    "\n",
    "# Convert the month floats to day ints and average by day (scale to [0,1], multiply by 364, add 1).\n",
    "curve_fit_df.index.values[:] = (364/11) * (curve_fit_df.index.values - 1) + 1\n",
    "curve_fit_df.index = curve_fit_df.index.astype(int)\n",
    "curve_fit_df.index.name = 'day of year'\n",
    "curve_fit_df = curve_fit_df.groupby('day of year').mean()\n",
    "\n",
    "# Export the data to a CSV.\n",
    "csv_output_dir = 'output/CSVs/'\n",
    "if not os.path.exists(csv_output_dir):\n",
    "    os.makedirs(csv_output_dir)\n",
    "curve_fit_df.to_csv(csv_output_dir + 'vegetation_phenology_yearly_curve_fits_landsat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span id=\"timesat\">Show [TIMESAT](http://web.nateko.lu.se/timesat/timesat.asp) Stats [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T14:41:28.018854Z",
     "iopub.status.busy": "2020-09-29T14:41:28.017397Z",
     "iopub.status.idle": "2020-09-29T14:41:28.020341Z",
     "shell.execute_reply": "2020-09-29T14:41:28.020749Z"
    }
   },
   "outputs": [],
   "source": [
    "def TIMESAT_stats(dataarray, time_dim='time'):\n",
    "    \"\"\"\n",
    "    For a 1D array of values for a vegetation index - for which higher values tend to \n",
    "    indicate more vegetation - determine several statistics:\n",
    "    1. Beginning of Season (BOS): The time index of the beginning of the growing season.\n",
    "        (The downward inflection point before the maximum vegetation index value)\n",
    "    2. End of Season (EOS): The time index of the end of the growing season.\n",
    "        (The upward inflection point after the maximum vegetation index value)\n",
    "    3. Middle of Season (MOS): The time index of the maximum vegetation index value.\n",
    "    4. Length of Season (EOS-BOS): The time length of the season (index difference).\n",
    "    5. Base Value (BASE): The minimum vegetation index value.\n",
    "    6. Max Value (MAX): The maximum vegetation index value (the value at MOS).\n",
    "    7. Amplitude (AMP): The difference between BASE and MAX.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataarray: xarray.DataArray\n",
    "        The 1D array of non-NaN values to determine the statistics for.\n",
    "    time_dim: string\n",
    "        The name of the time dimension in `dataarray`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stats: dict\n",
    "        A dictionary mapping statistic names to values.\n",
    "    \"\"\"\n",
    "    assert time_dim in dataarray.dims, \"The parameter `time_dim` is \\\"{}\\\", \" \\\n",
    "        \"but that dimension does not exist in the data.\".format(time_dim)\n",
    "    stats = {}\n",
    "    data_np_arr = dataarray.values\n",
    "    time_np_arr = dataarray[time_dim].values\n",
    "    data_inds = np.arange(len(data_np_arr))\n",
    "    \n",
    "    # Obtain the first and second derivatives.\n",
    "    fst_deriv = np.gradient(data_np_arr, time_np_arr)\n",
    "    pos_fst_deriv = fst_deriv > 0\n",
    "    neg_fst_deriv = 0 > fst_deriv\n",
    "    snd_deriv = np.gradient(fst_deriv, time_np_arr)\n",
    "    pos_snd_deriv = snd_deriv > 0\n",
    "    neg_snd_deriv = 0 > snd_deriv\n",
    "    \n",
    "    # Determine MOS.\n",
    "    # MOS is the index of the highest value immediately preceding a transition\n",
    "    # of the first derivative from positive to negative.\n",
    "    pos_to_neg_fst_deriv = pos_fst_deriv.copy()\n",
    "    for i in range(len(pos_fst_deriv)):\n",
    "        if i == len(pos_fst_deriv) - 1: # last index\n",
    "            pos_to_neg_fst_deriv[i] = False\n",
    "        elif pos_fst_deriv[i] and not pos_fst_deriv[i+1]: # + to -\n",
    "            pos_to_neg_fst_deriv[i] = True\n",
    "        else: # everything else\n",
    "            pos_to_neg_fst_deriv[i] = False\n",
    "    num_inflection_pts = pos_to_neg_fst_deriv.sum()\n",
    "    if num_inflection_pts > 0:\n",
    "        idxmos_potential_inds = data_inds[pos_to_neg_fst_deriv]\n",
    "        idxmos_subset_ind = np.argmax(data_np_arr[pos_to_neg_fst_deriv])\n",
    "        idxmos = idxmos_potential_inds[idxmos_subset_ind]\n",
    "    else:\n",
    "        idxmos = np.argmax(data_np_arr)\n",
    "    stats['Middle of Season'] = idxmos\n",
    "    \n",
    "    data_inds_after_mos = np.roll(data_inds, len(data_inds)-idxmos-1)\n",
    "    \n",
    "    # Determine BOS.\n",
    "    # BOS is the first negative inflection point of the positive values \n",
    "    # of the first derivative starting after and ending at the MOS.\n",
    "    idxbos = data_inds_after_mos[np.argmax((pos_fst_deriv & neg_snd_deriv)[data_inds_after_mos])]\n",
    "    stats['Beginning of Season'] = idxbos\n",
    "    \n",
    "    # Determine EOS.\n",
    "    # EOS is the last positive inflection point of the negative values \n",
    "    # of the first derivative starting after and ending at the MOS.\n",
    "    idxeos = data_inds_after_mos[np.argmax((neg_fst_deriv & pos_snd_deriv)[data_inds_after_mos][::-1])]\n",
    "    stats['End of Season'] = idxeos\n",
    "    \n",
    "    # Determine EOS-BOS.\n",
    "    stats['Length of Season'] = idxeos - idxbos\n",
    "    # Determine BASE.\n",
    "    stats['Base Value'] = data_np_arr.min()\n",
    "    # Determine MAX.\n",
    "    stats['Max Value'] = data_np_arr.max()\n",
    "    # Determine AMP.\n",
    "    stats['Amplitude'] = stats['Max Value'] - stats['Base Value']\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T14:41:28.029158Z",
     "iopub.status.busy": "2020-09-29T14:41:28.028549Z",
     "iopub.status.idle": "2020-09-29T14:41:28.031899Z",
     "shell.execute_reply": "2020-09-29T14:41:28.031476Z"
    }
   },
   "outputs": [],
   "source": [
    "## Settings\n",
    "# The minimum number of weeks or months with data for a year to have its stats calculated.\n",
    "# The aggregation used to obtain the plotting data determines which of these is used.\n",
    "min_weeks_per_year = 40\n",
    "min_months_per_year = 9\n",
    "## End Settings\n",
    "\n",
    "for year, dataarray in plotting_data_years.items():\n",
    "    dataarray = dataarray.mean(['latitude', 'longitude'])\n",
    "    non_nan_mask = ~np.isnan(dataarray.values)\n",
    "    num_times = non_nan_mask.sum()\n",
    "    insufficient_data = False\n",
    "    if bin_by == 'weekofyear':\n",
    "        if num_times < min_weeks_per_year:\n",
    "            print(\"There are {} weeks with data for the year {}, but the \" \\\n",
    "                  \"minimum number of weeks is {}.\\n\".format(num_times, year, min_weeks_per_year))\n",
    "            continue\n",
    "    elif bin_by == 'monthofyear':\n",
    "        if num_times < min_months_per_year:\n",
    "            print(\"There are {} months with data for the year {}, but the \" \\\n",
    "                  \"minimum number of months is {}.\\n\".format(num_times, year, min_months_per_year))\n",
    "            continue\n",
    "    # Remove NaNs for `TIMESAT_stats()`.\n",
    "    dataarray = dataarray.sel({time_dim_name: dataarray[time_dim_name].values[non_nan_mask]})\n",
    "    stats = TIMESAT_stats(dataarray, time_dim=time_dim_name)\n",
    "    # Map indices to days of the year (can't use data from `daysofyear_per_year` directly\n",
    "    # because `xarray_time_series_plot()` can have more points for smooth curve fitting.\n",
    "    time_int_arr = dataarray[time_dim_name].values\n",
    "    orig_day_int_arr = daysofyear_per_year[year].values\n",
    "    day_int_arr = np.interp(time_int_arr, (time_int_arr.min(), time_int_arr.max()), \n",
    "                            (orig_day_int_arr.min(), orig_day_int_arr.max()))\n",
    "    # Convert \"times\" in the TIMESAT stats from indices to days (ints).\n",
    "    stats['Beginning of Season'] = int(round(day_int_arr[stats['Beginning of Season']]))\n",
    "    stats['Middle of Season'] = int(round(day_int_arr[stats['Middle of Season']]))\n",
    "    stats['End of Season'] = int(round(day_int_arr[stats['End of Season']]))\n",
    "    stats['Length of Season'] = np.abs(stats['End of Season'] - stats['Beginning of Season']) \n",
    "    \n",
    "    print(\"Year =\", year)\n",
    "    print(\"Beginning of Season (BOS) day =\", stats['Beginning of Season'])\n",
    "    print(\"End of Season (EOS) day =\", stats['End of Season'])\n",
    "    print(\"Middle of Season (MOS) day =\", stats['Middle of Season'])\n",
    "    print(\"Length of Season (abs(EOS-BOS)) in days =\", stats['Length of Season'])\n",
    "    print(\"Base Value (Min) =\", stats['Base Value'])\n",
    "    print(\"Max Value (Max) =\", stats['Max Value'])\n",
    "    print(\"Amplitude (Max-Min) =\", stats['Amplitude'])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
