{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T16:13:29.354836Z",
     "iopub.status.busy": "2020-09-28T16:13:29.354401Z",
     "iopub.status.idle": "2020-09-28T16:13:29.650464Z",
     "shell.execute_reply": "2020-09-28T16:13:29.650938Z"
    }
   },
   "outputs": [],
   "source": [
    "# Enable importing of utilities\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyizing Rainfall near Lake Chad  \n",
    "\n",
    "This tutorial is focused on solving the problem of determining **when** the rainy season starts and ends over Lake Chad.\n",
    "[Future notebooks in this series]() deal with analyzing the Lake Chad region *before* and *after* the rainy season to determine how much the rainy season contributes to the surface area of Lake Chad. \n",
    "    \n",
    "\n",
    "### What to expect from this notebook\n",
    "\n",
    "- Introduction to precipitation data.\n",
    "- Exposure to datacube\n",
    "- Exposure to `xarrays`\n",
    "- Visualizing time series data \n",
    "- curve fitting to determine start and end dates of the rainy season. \n",
    "\n",
    "### Algorithmic process\n",
    "\n",
    "1. create Datacube object\n",
    "2. define boundaries of study area\n",
    "3. use boudaries to load data \n",
    "4. create a time series representation of data\n",
    "5. curve fit to find rainy season start and end\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the datacube object \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code connects to the datacube and accepts `chad_rainfall` as an app-name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T16:13:29.654812Z",
     "iopub.status.busy": "2020-09-28T16:13:29.654122Z",
     "iopub.status.idle": "2020-09-28T16:13:30.852090Z",
     "shell.execute_reply": "2020-09-28T16:13:30.851595Z"
    }
   },
   "outputs": [],
   "source": [
    "import datacube\n",
    "dc = datacube.Datacube(app = \"chad_rainfall\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object is the main interface to your stored and ingested data. It can handle complicated things like reprojecting data with varying resolutions and orientations. It can also be used to explore existing datasets. In this tutorial, it is only used for loading data from the datacube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loading GPM data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small dataset is easier to work with than the entirety of Lake Chad. The region you're about to load contains GPM measurements for a small area of Lake Chad near the mouth of its largest contributing river. The code below displays the bounds of the region but doesn't load it. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T16:13:30.855707Z",
     "iopub.status.busy": "2020-09-28T16:13:30.855243Z",
     "iopub.status.idle": "2020-09-28T16:13:31.037920Z",
     "shell.execute_reply": "2020-09-28T16:13:31.037418Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.dc_display_map import display_map\n",
    "display_map(latitude = (12.75, 13.0),longitude = (14.25, 14.5))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Setting boundaries of our load  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T16:13:31.045650Z",
     "iopub.status.busy": "2020-09-28T16:13:31.045188Z",
     "iopub.status.idle": "2020-09-28T16:13:31.047277Z",
     "shell.execute_reply": "2020-09-28T16:13:31.046843Z"
    }
   },
   "outputs": [],
   "source": [
    "## Define Geographic boundaries using a (min,max) tuple.\n",
    "latitude = (12.75, 13.0)\n",
    "longitude = (14.25, 14.5)\n",
    "\n",
    "## Specify a date range using a (min,max) tuple  \n",
    "from datetime import datetime\n",
    "time = (datetime(2015,1,1), datetime(2016,1,2))\n",
    "\n",
    "## define the name you gave your data while it was being \"ingested\", as well as the platform it was captured on. \n",
    "product = 'gpm_imerg_gis_daily_global'\n",
    "platform = 'GPM'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>  \n",
    "\n",
    "It's simple to intuit what the **latitude**,**longitude** and **time** bounds will get you. It will give you a bounded and grided-dataset containing our rainy season. Each square in the diagram below represents the smallest spatial unit in our imagery. This smallest unit is often reffered to as a **pixel**.\n",
    "  \n",
    "<br>  \n",
    "\n",
    "![img](../diagrams/rainy_demo/rainy2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While defining space and time bounds are simple to understand, it may be more complicated to pick up on what **product** and **platform** are. Platform will tell you how/where the data is produced. Product is a key used to chose what representation of that platform's data you wish to index.  \n",
    "\n",
    "For the sake of this tutorial, think of **product** and **platform** as shorthand names we used to look up data that is:  \n",
    "\n",
    "- produced on a **GPM** platform.\n",
    "  \n",
    "- represented using **gpm_imerg_gis_daily_global** settings or types.\n",
    "   \n",
    "The representation reflects personal prefferences that define, for example, the resolution of each pixel, how pixels are sampled, and the sort of geometric projections this grid of pixels undergoes to assume its current shape. Scripts for adding more general product types/representations are avaiable [here](), but aren't necessary to understanding this stage of the tutorial.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T16:13:31.059539Z",
     "iopub.status.busy": "2020-09-28T16:13:31.059117Z",
     "iopub.status.idle": "2020-09-28T16:13:31.175940Z",
     "shell.execute_reply": "2020-09-28T16:13:31.175107Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load Percipitation data using parameters,\n",
    "gpm_data = dc.load(latitude = latitude, longitude = longitude, time = time, product = product, platform = platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exploring precipitation data  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above should have loaded an [xarray]() containing your GPM data.  An xarray data-structure is essentially a wrapper for high-dimensional data-arrays. One of its main uses is the coupling of different data with a shared set of coordinates.\n",
    "  \n",
    "Conceptually, you can imagine GPM's xarray looking like this:  \n",
    "\n",
    "<br>\n",
    "\n",
    "![img](../diagrams/rainy_demo/gpm_01.png)  \n",
    "  \n",
    "<br>\n",
    "\n",
    "Each latitude-longitude coordinate pair will have,  `total_precipitation`, `liquid_precipitation`, `ice_precipitation` and `percent_liquid` measurements taken for them.  \n",
    "\n",
    "An Xarray Dataset will store each of these measurements in separate grids and share a single set of coordinates among all measurements.  \n",
    "\n",
    "To get some detailed information about an xarray, a `print()` statement will give you a readout on its structure.  \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( gpm_data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Using the readout above we can quickly gain a summary of the dataset we just loaded by examining:  \n",
    "\n",
    "- **Coordinates**  \n",
    "a readout of individual coordinates. In this case, it's three lists of `latitude`, `longitude`, and `time` values\n",
    "- **Dimensions**  \n",
    "a readout of how large each dimension is. In this case, we've loaded in a 3 by 3 area of land have 295 acquisitions between 2014-2015. This can hint that this is a daily, not hourly or monthly precipitation product.   \n",
    "- **Data Variables**  \n",
    "a readout of what sort of data is stored. In this case, each `latitude`, `longitude`, and `time` point will store four types of data. One for each `total_precipitation`, `liquid_precipitation`, `ice_precipitation`,`percent_liquid` variable.  \n",
    "\n",
    "- **Data Size**  \n",
    "Each entry has a size/type associated.  IE. `int32`, `int8`, `float64`.  You can use this to manage the memory footprint of your object. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the daily average of precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The xarray that you're using has several built-in functions to efficiently run large scale operations on all points within the xarray. The following code makes use of one of those built-in functions to determine the average precipitation in an area for each time slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_precipitation = gpm_data.mean(dim = ['latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will: \n",
    "-  take the average of all measurements in latitude rows and store them in a **time * longitude** coordinate pair. \n",
    "-  take the average of all measurements in longitude `longitude` rows and store them in a **time** coordinate. \n",
    "\n",
    "The diagram below should detail the operations of averaging these areas.  \n",
    "\n",
    "![img](../diagrams/rainy_demo/mean123.png)\n",
    "\n",
    "Our new xarray dataset will retain a single dimension called time. Each point in time will store a value representing the average of all pixel values at that time.  \n",
    "  \n",
    "Take a look at the print statement below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_precipitation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things you should notice:  \n",
    "\n",
    "- We're still representing mean_values using an xarray  \n",
    "\n",
    "- only time is displayed under the **coordinates** section.     \n",
    "  **latitude** and **longitude** are essentially dropped.  \n",
    "  \n",
    "- The averaging operation was performed on all datasets; `total_precipitation`, `liquid_precipitation`,`ice_precipitation`, and `percent_liquid`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xarray Datasets store several [data-arrays](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.html).  \n",
    "\n",
    "The code below neatly extracts a **total_precipitation** data-array from our **mean_precipitation** Dataset.  \n",
    "<br>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_total_precipitation = mean_precipitation.total_precipitation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>  \n",
    "The new representation is also an xarray data-structure. When printed it looks something like this.  \n",
    "<br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_total_precipitation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "For time series plotting we care about extracting **time** coordinates, and the data-array **values**  \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = mean_total_precipitation.time.values\n",
    "values = mean_total_precipitation.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>  \n",
    "The next line of code plots a time series graph of our values  \n",
    "<br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(times, values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the bounds of the rainy season  \n",
    "  \n",
    "The section above displayed daily precipitation values observed in 2015.  \n",
    "The shape would fit a bell curve very well.   The following code deals with fitting a bell curve to our time series.  \n",
    "<br>  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for this algorithm is out of scope for this tutorial on datacube and is abstracted away for demonstration purposes. \n",
    "import demo.curve_fit_gaussian as curve_fit\n",
    "\n",
    "curve_fit.plot_fit(times, values, standard_deviations = 2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>  \n",
    "We pick two points that are equidistant from the center/peak of this curve to act as our bounding points for the rainy season.  \n",
    "<br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_fit.get_bounds(times, values, standard_deviations = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that **JUNE** and **OCTOBER** should be adequate bounds for the rainy season.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Next Steps  \n",
    "\n",
    "This notebook served as an introduction to datacube and the xarray datasets. Now that we have the extent of our rainy season you can proceed with the [next notebook](igarss_chad_02.ipynb), in which landsat 7 data is broken into a pre and post rainy season dataset and cleaned up in preparation for [water detection](igarss_chad_03.ipynb). The entire notebook has been condensed down to a about a dozen lines of code below.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import demo.curve_fit_gaussian as curve_fit\n",
    "\n",
    "latitude = (12.75, 13.0)\n",
    "longitude = (14.25, 14.5)\n",
    "time = (datetime(2015,1,1), datetime(2016,1,2))\n",
    "product = 'gpm_imerg_gis_daily_global'\n",
    "platform = 'GPM'  \n",
    "gpm_data = dc.load(latitude = latitude, longitude = longitude, time = time, product = product, platform = platform)\n",
    "\n",
    "mean_precipitation = gpm_data.mean(dim = ['latitude', 'longitude'])\n",
    "\n",
    "times =  mean_precipitation.time.values\n",
    "values = mean_precipitation.total_precipitation.values  \n",
    "  \n",
    "curve_fit.get_bounds(times, values, standard_deviations = 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
