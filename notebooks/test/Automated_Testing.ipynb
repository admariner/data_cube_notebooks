{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "from nbconvert.preprocessors import CellExecutionError\n",
    "from nbclient.exceptions import CellControlSignal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a list of notebooks to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "root_path = os.environ.get('NOTEBOOK_ROOT')\n",
    "sys.path.append(root_path)\n",
    "\n",
    "# exclude 'AMA', 'experimental', 'IGARSS', and 'legacy' \n",
    "#   because these directories are not intended for public release.\n",
    "# exclude 'DEM' and 'SAR' directories because the testing environment does not have the necessary data.\n",
    "# exclude 'machine_learning' because we do not have the data\n",
    "#   to test the Uruguay Random Forest notebooks.\n",
    "# exclude 'UN_SDG_15_3_1.ipynb' due to no land change data (instructions in nbk).\n",
    "# exclude 'ALOS_Land_Change.ipynb' due to no ALOS data.\n",
    "# exclude 'Landslide_Identification_SLIP.iypnb' due to no TERRA ASTER data.\n",
    "# exclude 'Forest_Change_VNSC.ipynb' due to no TERRA ASTER data.\n",
    "# exclude 'Shallow_Water_Bathymetry.ipynb' due to no Landsat 8 Level 1 data.\n",
    "# exclude 'ALOS_WASARD.ipynb' due to no ALOS data.\n",
    "# exclude 'water_interoperability_similarity.ipynb' due to no Sentinel-2 data.\n",
    "exclude_subpaths = ['.ipynb_checkpoints', 'AMA', 'DEM', 'SAR', 'experimental', 'IGARSS', \n",
    "                    'legacy', 'machine_learning', 'test', 'utils', \n",
    "                    'Forest_Change_VNSC.ipynb', 'Shallow_Water_Bathymetry.ipynb',\n",
    "                    'Automated_Testing.ipynb', 'UN_SDG_15_3_1.ipynb',\n",
    "                    'ALOS_Land_Change.ipynb', 'Landslide_Identification_SLIP.ipynb',\n",
    "                    'Forest_Change_VNSC.ipynb', 'Shallow_Water_Bathymetry.ipynb',\n",
    "                    'ALOS_WASARD.ipynb', 'water_interoperability_similarity.ipynb',\n",
    "                    ]\n",
    "notebook_file_paths = []\n",
    "for root, directories, files in os.walk('..', topdown=True):\n",
    "    notebook_file_paths.extend([os.path.join(root, file) for file in files if file.endswith('.ipynb')])\n",
    "notebook_file_paths = sorted(notebook_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter out notebooks using `exclude_subpaths`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_filepath_excluded(filepath, exclude_subpaths):\n",
    "    return any(list(map(lambda subpath: subpath in filepath, exclude_subpaths)))\n",
    "notebook_file_paths = [filepath for filepath in notebook_file_paths if not is_filepath_excluded(filepath, exclude_subpaths)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the notebooks and record their status (e.g. working, error) to HTML as each completes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas\n",
    "ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n",
    "full_report = pandas.DataFrame(columns=['Notebook', 'Status', 'Bug Description'])\n",
    "error_report = pandas.DataFrame(columns=['Notebook', 'Status', 'Bug Description'])\n",
    "success_report = pandas.DataFrame(columns=['Notebook', 'Status', 'Bug Description'])\n",
    "# Unless this cell is rerun, only run notebooks that have not been run successfully yet.\n",
    "success_notebooks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To rerun all notebooks, the file `success_nbks_file_name` must be deleted and then this notebook must be restarted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ../vegetation/phenology/Vegetation_Phenology.ipynb\n",
      "Running ../water/detection/water_detection_with_wofs.ipynb\n",
      "Running ../water/quality/TSM_Demo.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the list of successful notebooks.\n",
    "success_nbks_file_name = 'success_nbks.pkl'\n",
    "if os.path.exists(success_nbks_file_name):\n",
    "    success_nbks_file_in = open(success_nbks_file_name, 'rb')\n",
    "    success_notebooks = pickle.load(success_nbks_file_in)\n",
    "\n",
    "notebook_file_paths_to_run = [nbk_pth for nbk_pth in notebook_file_paths if nbk_pth not in success_notebooks]\n",
    "for notebook_file_path in notebook_file_paths_to_run:\n",
    "    print(f'Running {notebook_file_path}')\n",
    "    run_result = {'Notebook': notebook_file_path, 'Status': 'Working', 'Bug Description': ''}\n",
    "    with open(notebook_file_path, 'r+', encoding='utf-8') as notebook_file:\n",
    "        notebook = nbformat.read(notebook_file, as_version=4)\n",
    "        notebook_runner = ExecutePreprocessor(timeout=None)\n",
    "        try:\n",
    "            notebook_runner.preprocess(notebook, {'metadata': {'path': os.path.dirname(notebook_file_path)}})\n",
    "        except CellExecutionError as err:\n",
    "            run_result['Status'] = 'Error'\n",
    "            run_result['Bug Description'] = err\n",
    "        error = run_result['Status'] == 'Error'\n",
    "        # Save the notebook.\n",
    "#         nbformat.write(notebook, notebook_file_path)\n",
    "    full_report = full_report.append(run_result, ignore_index=True)\n",
    "    if error:\n",
    "        error_report = error_report.append(run_result, ignore_index=True)\n",
    "    else:\n",
    "        # Record that this notebook ran successfully to avoid running it again for this testing session.\n",
    "        success_notebooks.append(notebook_file_path)\n",
    "        success_report = success_report.append(run_result, ignore_index=True)\n",
    "        success_nbks_file_out = open(success_nbks_file_name, 'wb') \n",
    "        pickle.dump(success_notebooks, success_nbks_file_out)\n",
    "    full_report.to_html('full_test_report.html', escape=False, formatters={'Bug Description': lambda e: ansi_escape.sub('', str(e).replace('\\n', '<br/>'))})\n",
    "    error_report.to_html('error_report.html', escape=False, formatters={'Bug Description': lambda e: ansi_escape.sub('', str(e).replace('\\n', '<br/>'))})\n",
    "    success_report.to_html('success_report.html', escape=False, formatters={'Bug Description': lambda e: ansi_escape.sub('', str(e).replace('\\n', '<br/>'))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the results to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_report['Bug Description'] = full_report['Bug Description'].map(lambda e: ansi_escape.sub('', str(e)))\n",
    "full_report.to_csv('full_test_report.csv')\n",
    "error_report['Bug Description'] = error_report['Bug Description'].map(lambda e: ansi_escape.sub('', str(e)))\n",
    "error_report.to_csv('error_report.csv')\n",
    "success_report['Bug Description'] = success_report['Bug Description'].map(lambda e: ansi_escape.sub('', str(e)))\n",
    "success_report.to_csv('success_report_report.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
