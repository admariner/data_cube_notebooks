{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# UN SDG Indicator 6.6.1:<br> Change in the Extent of Water-related Ecosystems\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "# Notebook Summary\n",
    "The United Nations have prescribed 17 \"Sustainable Development Goals\" (SDGs). This notebook attempts to monitor SDG Indicator 6.6.1 - change in the extent of water-related ecosystems. Indicator 6.6.1 has 4 sub-indicators:\n",
    ">    i. The spatial extent of water-related ecosystems <br>\n",
    ">    ii. The quantity of water contained within these ecosystems <br>\n",
    ">    iii. The quality of water within these ecosystems <br>\n",
    ">    iv. The health or state of these ecosystems <br>\n",
    "\n",
    "This notebook primarily focuses on the first sub-indicator - spatial extents.\n",
    "\n",
    "In the first section, one time period is analyzed. The mean of water classifications over time is visualized, the minimum and maximum water extents are visualized, and the frequency of water classification for each pixel is visualized.\n",
    "\n",
    "In the second section, two time periods are analyzed - a baseline time period and an analysis time period. The change in pixels that are water at any time during their respective time periods is calculated, the change in water from the baseline time period to the analysis time period is visualized, and the area is classified by degree of change using classification labels provided by UN SDG 6.6.1.\n",
    "\n",
    "<hr>\n",
    "\n",
    "# Index\n",
    "\n",
    "* [Import Dependencies and Connect to the Data Cube](#import)\n",
    "* [Choose Platforms and Products](#plat_prod)\n",
    "* [Get the Extents of the Cube](#extents)\n",
    "* [Analyze a Single Time Period](#single_period)\n",
    "    * [Define the Extents of the Analysis](#define_extents_single)\n",
    "    * [Retrieve the Data from the Datacube](#retrieve_data_single)\n",
    "    * [Get Water Classifications Using the WOFS Algorithm](#water_cls_single)\n",
    "    * [Prepare for Visualization](#prepare_for_vis)\n",
    "    * [Find the Minimum and Maximum Water Extents](#find_water_extents)\n",
    "    * [Create Water Extent Image](#water_extent_image)\n",
    "    * [Create a Time Series Plot of the Water](#time_series_water)\n",
    "* [Compare Two Time Periods - a Baseline and an Analysis](#two_period)\n",
    "    * [Define the Extents of the Analysis](#define_extents_baseline_analysis)\n",
    "    * [Retrieve the Data from the Datacube](#retrieve_data_baseline_analysis)\n",
    "    * [Get Water Classifications Using the WOFS Algorithm](#water_cls_baseline_analysis)\n",
    "    * [Show and Analyze the Differences Between the Two Time Periods](#show_analyze_diff)\n",
    "    * [Classify the Area by Degree of Change](#classify_by_degree_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"import\"></a>Import Dependencies and Connect to the Data Cube [&#9652;](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:56:46.590195Z",
     "iopub.status.busy": "2020-09-29T00:56:46.589741Z",
     "iopub.status.idle": "2020-09-29T00:56:46.591541Z",
     "shell.execute_reply": "2020-09-29T00:56:46.591971Z"
    }
   },
   "outputs": [],
   "source": [
    "# Supress Warning \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:56:46.597119Z",
     "iopub.status.busy": "2020-09-29T00:56:46.596700Z",
     "iopub.status.idle": "2020-09-29T00:56:47.754292Z",
     "shell.execute_reply": "2020-09-29T00:56:47.754748Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Allow importing of our utilities.\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.environ.get('NOTEBOOK_ROOT'))\n",
    "\n",
    "# Import the datacube and the API\n",
    "import datacube\n",
    "from utils.data_cube_utilities.data_access_api import DataAccessApi\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:56:47.758621Z",
     "iopub.status.busy": "2020-09-29T00:56:47.758198Z",
     "iopub.status.idle": "2020-09-29T00:56:48.072052Z",
     "shell.execute_reply": "2020-09-29T00:56:48.072505Z"
    }
   },
   "outputs": [],
   "source": [
    "from datacube.utils.aws import configure_s3_access\n",
    "configure_s3_access(requester_pays=True)\n",
    "\n",
    "# Create an instance of the datacube and API.\n",
    "api = DataAccessApi()\n",
    "dc = api.dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"plat_prod\"></a>Choose Platforms and Products [&#9652;](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List available products for each platform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:56:48.076566Z",
     "iopub.status.busy": "2020-09-29T00:56:48.076052Z",
     "iopub.status.idle": "2020-09-29T00:56:48.107151Z",
     "shell.execute_reply": "2020-09-29T00:56:48.107570Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get available products\n",
    "products_info = dc.list_products()\n",
    "\n",
    "# List LANDSAT 7 products\n",
    "print(\"LANDSAT 7 Products:\")\n",
    "products_info[[\"platform\", \"name\"]][products_info.platform == \"LANDSAT_7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:56:48.114580Z",
     "iopub.status.busy": "2020-09-29T00:56:48.114160Z",
     "iopub.status.idle": "2020-09-29T00:56:48.116927Z",
     "shell.execute_reply": "2020-09-29T00:56:48.117336Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List LANDSAT 8 products\n",
    "print(\"LANDSAT 8 Products:\")\n",
    "products_info[[\"platform\", \"name\"]][products_info.platform == \"LANDSAT_8\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose products**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:56:48.121684Z",
     "iopub.status.busy": "2020-09-29T00:56:48.121229Z",
     "iopub.status.idle": "2020-09-29T00:56:48.122856Z",
     "shell.execute_reply": "2020-09-29T00:56:48.123261Z"
    }
   },
   "outputs": [],
   "source": [
    "# These are the platforms (satelltes) and products (datacube sets) \n",
    "# used for this demonstration.\n",
    "# Single time period analysis\n",
    "platforms_single = ['LANDSAT_8']\n",
    "products_single = ['ls8_usgs_sr_scene']\n",
    "collections_single = ['c1']\n",
    "levels_single = ['l2']\n",
    "# Two time period analysis\n",
    "platforms_two = ['LANDSAT_7', 'LANDSAT_8']\n",
    "products_two = ['ls7_usgs_sr_scene', 'ls8_usgs_sr_scene']\n",
    "collections_two = ['c1', 'c1']\n",
    "levels_two = ['l2', 'l2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"extents\"></a>Get the Extents of the Cube [&#9652;](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:56:48.128390Z",
     "iopub.status.busy": "2020-09-29T00:56:48.127966Z",
     "iopub.status.idle": "2020-09-29T00:56:59.822436Z",
     "shell.execute_reply": "2020-09-29T00:56:59.822864Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.dc_load import get_overlapping_area\n",
    "from utils.data_cube_utilities.dc_time import dt_to_str\n",
    "\n",
    "full_lat, full_lon, min_max_dates = \\\n",
    "    get_overlapping_area(api, platforms_two, products_two)\n",
    "\n",
    "# Print the extents of each product.\n",
    "str_min_max_dates = np.vectorize(dt_to_str)(min_max_dates)\n",
    "for i, (platform, product) in enumerate(zip(platforms_two, products_two)):\n",
    "    print(\"For platform {} and product {}:\".format(platform, product))\n",
    "    print(\"Time Extents:\", str_min_max_dates[i])\n",
    "    print()\n",
    "\n",
    "# Print the extents of the combined data.\n",
    "min_start_date_mutual = np.max(min_max_dates[:,0])\n",
    "max_end_date_mutual = np.min(min_max_dates[:,1])\n",
    "print(\"Overlapping Extents:\")\n",
    "print(\"Latitude Extents:\", full_lat)\n",
    "print(\"Longitude Extents:\", full_lon)\n",
    "print(\"Time Extents:\", list(map(dt_to_str, (min_start_date_mutual, max_end_date_mutual))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the full area**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:56:59.826078Z",
     "iopub.status.busy": "2020-09-29T00:56:59.825655Z",
     "iopub.status.idle": "2020-09-29T00:57:00.000625Z",
     "shell.execute_reply": "2020-09-29T00:57:00.000963Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.dc_display_map import display_map\n",
    "display_map(full_lat, full_lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"single_period\"></a>Analyze a Single Time Period  [&#9652;](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"define_extents_single\"></a>Define the Extents of the Analysis [&#9652;](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify start and end dates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:00.007949Z",
     "iopub.status.busy": "2020-09-29T00:57:00.007533Z",
     "iopub.status.idle": "2020-09-29T00:57:00.009796Z",
     "shell.execute_reply": "2020-09-29T00:57:00.009321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select a subset of the time available (year, month, day).\n",
    "time_extents_single = [dt.datetime(2014,1,1), dt.datetime(2014,1,31)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify an area to analyze**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:00.013310Z",
     "iopub.status.busy": "2020-09-29T00:57:00.012878Z",
     "iopub.status.idle": "2020-09-29T00:57:00.015021Z",
     "shell.execute_reply": "2020-09-29T00:57:00.014595Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify latitude and longitude bounds of an interesting area within the full extents.\n",
    "\n",
    "# Ghana\n",
    "# lat = (6.5016, 6.5221) # Lake Volta (very small)\n",
    "# lon = (-0.1618, -0.100) # Lake Volta (very small)\n",
    "# lat = (6.5016, 6.5821) # Lake Volta (small)\n",
    "# lon = (-0.1618, -0.055) # Lake Volta (small)\n",
    "# lat = (6.5138, 6.5292) # Lake Volta (medium)\n",
    "# lon = (-0.1669, -0.1493) # Lake Volta (medium)\n",
    "# lat = (6.2989, 7.9287) # Lake Volta (large)\n",
    "# lon = (-0.4559, 0.2637) # Lake Volta (large)\n",
    "# lat =  (10.8600, 10.9350) # Tono Dam (old)\n",
    "# lon = (-1.1934, -1.1423) # Tono Dam (old)\n",
    "lat =  (10.8622, 10.9143) # Tono Dam\n",
    "lon = (-1.1822, -1.1440) # Tono Dam\n",
    "\n",
    "# Lake Sulunga, Tanzania\n",
    "# lat = (-6.3605, -5.8252) # Large \n",
    "# lon = (34.9756, 35.4535) # Large\n",
    "# lat = (-6.2593, -5.8701) # Small (close fit)\n",
    "# lon = (34.9901, 35.3641) #  Small (close fit)\n",
    "\n",
    "# Lake Manyara, Tanzania\n",
    "# lat = (-3.8505, -3.3886) \n",
    "# lon = (35.7184, 35.9271)\n",
    "\n",
    "# Vietnam\n",
    "# lat = (10.037934, 10.237934) \n",
    "# lon = (104.992264, 105.192264)\n",
    "# lat = (11.0645, 11.2845) # Tri An Lake\n",
    "# lon = (106.9567, 107.2967) # Tri An Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the selected area**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:00.018402Z",
     "iopub.status.busy": "2020-09-29T00:57:00.018002Z",
     "iopub.status.idle": "2020-09-29T00:57:00.026538Z",
     "shell.execute_reply": "2020-09-29T00:57:00.026113Z"
    }
   },
   "outputs": [],
   "source": [
    "display_map(lat, lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"retrieve_data_single\"></a>Retrieve the Data from the Datacube [&#9652;](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:00.036228Z",
     "iopub.status.busy": "2020-09-29T00:57:00.035523Z",
     "iopub.status.idle": "2020-09-29T00:57:00.039006Z",
     "shell.execute_reply": "2020-09-29T00:57:00.038569Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.dc_load import match_dim_sizes\n",
    "from utils.data_cube_utilities.clean_mask import landsat_clean_mask_full\n",
    "from utils.data_cube_utilities.aggregate import xr_scale_res\n",
    "from utils.data_cube_utilities.sort import xarray_sortby_coord\n",
    "\n",
    "def load_for_time_range(platforms, products, collections, levels, time_extents):\n",
    "    measurements = ['red', 'blue', 'green', 'nir', 'swir1', 'swir2', 'pixel_qa']\n",
    "    matching_abs_res, same_dim_sizes = match_dim_sizes(dc, products, lon, lat)\n",
    "    datasets = {}\n",
    "    clean_masks = {}\n",
    "    for platform, product, collection, level in zip(platforms, products, collections, levels):\n",
    "        # Load the dataset.\n",
    "        prod_info = dc.list_products()\n",
    "        resolution = prod_info[prod_info.name==product].resolution.values[0]\n",
    "        dataset = dc.load(platform=platform, product=product, lat=lat, lon=lon, \n",
    "                          time=time_extents, measurements=measurements,\n",
    "                          group_by='solar_day', \n",
    "                          output_crs='EPSG:4326',\n",
    "                          resolution=resolution,\n",
    "                          dask_chunks={'latitude':1000, 'longitude':1000, 'time':1}).persist()\n",
    "        if len(dataset.dims) == 0: # The dataset is empty.\n",
    "            continue\n",
    "        # Get the clean mask.\n",
    "        clean_mask = landsat_clean_mask_full(dc, dataset, product, platform, collection, level)\n",
    "        dataset = dataset.drop('pixel_qa')\n",
    "        # Discard acquisitions with no clean data.\n",
    "        acq_times_to_keep = dataset.time.isel(time=(clean_mask.mean(['latitude', 'longitude']) > 0.001))\n",
    "        dataset = dataset.sel(time=acq_times_to_keep)\n",
    "        clean_mask = clean_mask.sel(time=acq_times_to_keep)\n",
    "        # If needed, scale the datasets and clean masks to the same size in the x and y dimensions.\n",
    "        if not same_dim_sizes:    \n",
    "            dataset = xr_scale_res(dataset, abs_res=matching_abs_res)\n",
    "            clean_mask = xr_scale_res(clean_mask.astype(np.uint8), abs_res=matching_abs_res).astype(np.bool)\n",
    "        # Clean the data.\n",
    "        dataset = dataset.astype(np.float16).where(clean_mask)\n",
    "        datasets[product], clean_masks[product] = dataset, clean_mask\n",
    "    # Combine everything.\n",
    "    if len(datasets) > 0:\n",
    "        dataset = xarray_sortby_coord(xr.concat(list(datasets.values()), dim='time'), coord='time')\n",
    "        clean_mask = xarray_sortby_coord(xr.concat(list(clean_masks.values()), dim='time'), coord='time')\n",
    "    else:\n",
    "        dataset = xr.Dataset()\n",
    "        clean_mask = xr.DataArray(np.empty((0,), dtype=np.bool))\n",
    "    return dataset, clean_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:00.043040Z",
     "iopub.status.busy": "2020-09-29T00:57:00.042381Z",
     "iopub.status.idle": "2020-09-29T00:57:05.107349Z",
     "shell.execute_reply": "2020-09-29T00:57:05.106844Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset, clean_mask = load_for_time_range(platforms_single, products_single, \n",
    "                                          collections_single, levels_single, time_extents_single)\n",
    "dataset = dataset.persist()\n",
    "clean_mask = clean_mask.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"water_cls_single\"></a> Get Water Classifications Using the WOFS Algorithm [&#9652;](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:05.110955Z",
     "iopub.status.busy": "2020-09-29T00:57:05.110527Z",
     "iopub.status.idle": "2020-09-29T00:57:05.112765Z",
     "shell.execute_reply": "2020-09-29T00:57:05.112328Z"
    }
   },
   "outputs": [],
   "source": [
    "# Whether or not to denoise the WOFS classifications.\n",
    "denoise = False\n",
    "\n",
    "# Denoising parameters may need to be tuned depending on the region and resolution scaling.\n",
    "# *min_size*: Minimum size of connected pixels \n",
    "# that will not be filtered (minimum=2).\n",
    "# *kernel_size*: Diameter of the modal kernal (minimum=3). \n",
    "# Larger values will run slower as it calculates the mode \n",
    "# of the neighborhood for each pixel inside this diameter.\n",
    "# *connectivity*: Maximum distance between any two pixels. \n",
    "# A value of 1 allows only contiguous regions of pixels.\n",
    "# *max_num_filter_runs*: The filter is run until the output stops changing \n",
    "# or this number of runs has been performed. \n",
    "std_denoise_params = dict(min_size=40, kernel_size=7, connectivity=10)\n",
    "max_num_filter_runs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:05.119351Z",
     "iopub.status.busy": "2020-09-29T00:57:05.118671Z",
     "iopub.status.idle": "2020-09-29T00:57:05.256771Z",
     "shell.execute_reply": "2020-09-29T00:57:05.256279Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.dc_water_classifier import wofs_classify\n",
    "from utils.data_cube_utilities.dc_utilities import ignore_warnings\n",
    "from utils.data_cube_utilities.raster_filter import lone_object_filter\n",
    "\n",
    "def get_water_classifications(dataset, clean_mask, max_num_filter_runs, \n",
    "                              denoise=False, denoise_params=None):\n",
    "    # Get WOFS classifications.\n",
    "    water_da = ignore_warnings(wofs_classify, dataset).wofs\n",
    "    water_da = water_da.where(clean_mask)\n",
    "\n",
    "    # Denoise the classifications.\n",
    "    if denoise:\n",
    "        denoise_params = std_denoise_params if denoise_params is None else denoise_params\n",
    "        \n",
    "        # Encode NaN as -1 for `lone_object_filter()`.\n",
    "        water_da = water_da.where(~xr_nan(water_da), -1)\n",
    "        for time in water_da.time:\n",
    "            water_slice_prev = water_da.sel(time=time).values\n",
    "            water_slice_new = lone_object_filter(water_slice_prev, **std_denoise_params)\n",
    "            # While the filter output is still changing or until the filter has\n",
    "            # been run `max_num_filter_runs` times, keep rerunning it.\n",
    "            filter_run_count = 0\n",
    "            while (water_slice_new != water_slice_prev).any() and \\\n",
    "                  filter_run_count < max_num_filter_runs:\n",
    "                water_slice_prev = water_slice_new\n",
    "                water_slice_new = lone_object_filter(water_slice_prev, **std_denoise_params)\n",
    "                filter_run_count += 1\n",
    "            water_da.sel(time=time).values[:] = water_slice_new\n",
    "        water_da = water_da.where(water_da!=-1) # Reencode -1 as NaN.  \n",
    "    return water_da\n",
    "\n",
    "water_da = get_water_classifications(dataset, clean_mask, max_num_filter_runs,\n",
    "                                     denoise, std_denoise_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:05.261825Z",
     "iopub.status.busy": "2020-09-29T00:57:05.261140Z",
     "iopub.status.idle": "2020-09-29T00:57:07.591258Z",
     "shell.execute_reply": "2020-09-29T00:57:07.590764Z"
    }
   },
   "outputs": [],
   "source": [
    "# Resample the data in time to avoid cluttering the x axes of plots \n",
    "# with tick labels (dates) and also smooth the plots.\n",
    "dataset = dataset.resample(time='2d').mean('time').dropna('time', how='all').persist()\n",
    "clean_mask = (clean_mask.resample(time='2d').mean('time').dropna('time', how='all') >= 0.5).persist()\n",
    "water_da = water_da.resample(time='2d').mean('time').dropna('time', how='all').persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"prepare_for_vis\"></a> Prepare for Visualization [&#9652;](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:07.594524Z",
     "iopub.status.busy": "2020-09-29T00:57:07.594083Z",
     "iopub.status.idle": "2020-09-29T00:57:07.596175Z",
     "shell.execute_reply": "2020-09-29T00:57:07.595736Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a standard dpi partly to tune font sizes in figures.\n",
    "# A higher dpi and a lower figure size will result in larger \n",
    "# font sizes relative to the figure size.\n",
    "std_dpi = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"find_water_extents\"></a>**Find the Minimum and Maximum Water Extents** [&#9652;](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are finding the minimum and maximum water extents based on a max-composite of water data. We do this to reduce the appearance of missing data due to cloud cover and to avoid cluttering a plot of the mean of water over time later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:07.619199Z",
     "iopub.status.busy": "2020-09-29T00:57:07.618476Z",
     "iopub.status.idle": "2020-09-29T00:57:07.754320Z",
     "shell.execute_reply": "2020-09-29T00:57:07.754787Z"
    }
   },
   "outputs": [],
   "source": [
    "## Settings ##\n",
    "\n",
    "# Formatting for full-width plots.\n",
    "title_fontdict = dict(fontsize=16) # Title formatting\n",
    "tick_label_fmt_dict = dict(axis='both', labelsize=12) # Tick label formatting\n",
    "axis_label_fmt_dict = dict(fontsize=16) # Axis label formatting\n",
    "legend_kwargs = dict(fontsize=12)\n",
    "\n",
    "# Can be any of ['time-slice', 'per-pixel'].\n",
    "# The value 'time-slice' obtains the minimum and maximum water extents on an time-slice basis.\n",
    "# The value 'per-pixel' obtains the minumum and maximum water extents on a per-pixel basis.\n",
    "water_extent_method = 'time-slice'\n",
    "assert water_extent_method in ['time-slice', 'per-pixel'], \\\n",
    "    \"The setting `water_extent_method` must be one of ['time-slice', 'per-pixel'].\"\n",
    "\n",
    "# This setting is only relevant if `water_extent_method` is set to 'time-slice'.\n",
    "# Water classifiers can sometimes output erroneous classifications and sometimes excessive clouds\n",
    "# make compositing an impractical solution to accounting for noisy classifications. \n",
    "# And SAR data is sometimes unavailable for WASARD water classification over cloudy regions,\n",
    "# so percentiles can be specified for the minimum and maximum time slices. These can be between\n",
    "# 0 and 100. The minimum water time-slice selected will be the time slice with non-water extents\n",
    "# at or below `percentile_min`. The maximum water time-slice selected will be the time slice \n",
    "# with water extents at or below `percentile_max`.\n",
    "percentile_min, percentile_max = 100, 100\n",
    "\n",
    "## End Settings ##\n",
    "\n",
    "# Option 1: Obtain the acquisitions with the minimum and maximum water extents.\n",
    "if water_extent_method == 'time-slice':\n",
    "    count_non_water = water_da.where(water_da == 0).count(dim=['latitude', 'longitude'])\n",
    "    count_water = water_da.where(water_da == 1).count(dim=['latitude', 'longitude'])\n",
    "    \n",
    "    # Find the acquisition with the least water (the one with the most non-water pixels).\n",
    "    percentile_value_min = np.percentile(count_non_water, percentile_min, interpolation='lower')\n",
    "    # Handle the case of multiple acquisitions with the same amount of non-water (notably 0).\n",
    "    acqs_with_value_min = count_non_water == percentile_value_min\n",
    "    if acqs_with_value_min.sum() > 1: # Find the acquisition with the least water (and most non-water).\n",
    "        min_extent_time_ind = np.argmin(count_water.isel(time=acqs_with_value_min).values)\n",
    "    else:\n",
    "        min_extent_time_ind = (count_non_water == percentile_value_min).argmax().values\n",
    "    min_extent_time = water_da.time.values[min_extent_time_ind]\n",
    "    min_water_extent = water_da.isel(time=min_extent_time_ind)\n",
    "\n",
    "    # Find the acquisition with the most water (the one with the most water pixels).\n",
    "    percentile_value_max = np.percentile(count_water, percentile_max, interpolation='lower')\n",
    "    # Handle the case of multiple acquisitions with the same amount of water (notably 0).\n",
    "    acqs_with_value_max = count_water == percentile_value_max\n",
    "    if acqs_with_value_max.sum() > 1: # Find the acquisition with the least non-water (and most water).\n",
    "        max_extent_time_ind = np.argmin(count_non_water.isel(time=acqs_with_value_max).values)\n",
    "    else:\n",
    "        max_extent_time_ind = (count_water == percentile_value_max).argmax().values\n",
    "    max_extent_time = water_da.time.values[max_extent_time_ind]\n",
    "    max_water_extent = water_da.isel(time=max_extent_time_ind)\n",
    "    \n",
    "    dt64_to_date_str = lambda dt64 : pd.to_datetime(str(dt64)).strftime('%Y/%m/%d')\n",
    "    max_water_date_str = dt64_to_date_str(max_extent_time)\n",
    "    min_water_date_str = dt64_to_date_str(min_extent_time)\n",
    "    \n",
    "# Option 2: Obtain the minimum and maximum water extents on a per-pixel basis.\n",
    "else: \n",
    "    min_water_extent = water_da.min('time')\n",
    "    max_water_extent = water_da.max('time')\n",
    "    max_water_date_str = \"N/A\"\n",
    "    min_water_date_str = \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:07.762048Z",
     "iopub.status.busy": "2020-09-29T00:57:07.761599Z",
     "iopub.status.idle": "2020-09-29T00:57:08.304495Z",
     "shell.execute_reply": "2020-09-29T00:57:08.304921Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.plotter_utils import create_discrete_color_map, xarray_imshow\n",
    "from utils.data_cube_utilities.plotter_utils import figure_ratio\n",
    "\n",
    "# Begin plotting the data\n",
    "fig_width = 12 # Use this to change the size of this figure.\n",
    "fig = plt.figure(figsize=figure_ratio(water_da, fixed_width=fig_width), dpi=std_dpi)\n",
    "\n",
    "# Create common colormap.\n",
    "cmap = create_discrete_color_map([0,1], ['black', 'cyan'])\n",
    "\n",
    "imshow_kwargs = dict(vmin=0, vmax=1, cmap=cmap)\n",
    "\n",
    "legend_labels = {0:'Not Water', 1:'Water'}\n",
    "\n",
    "# Minimum water extent\n",
    "ax = plt.subplot(1,2,1)\n",
    "fig, ax, im, cbar = \\\n",
    "    xarray_imshow(min_water_extent, fig=fig, ax=ax, use_colorbar=False, use_legend=True, \n",
    "                  legend_labels=legend_labels, imshow_kwargs=imshow_kwargs, \n",
    "                  x_label_kwargs=axis_label_fmt_dict, y_label_kwargs=axis_label_fmt_dict,\n",
    "                  legend_kwargs=legend_kwargs)\n",
    "ax.set_title(\"Minimum Water Extent \\nTime (YYYY/MM/DD): {}\".format(min_water_date_str), fontdict=title_fontdict)\n",
    "ax.tick_params(**tick_label_fmt_dict)\n",
    "\n",
    "# Maximum water extent\n",
    "ax = plt.subplot(1,2,2)\n",
    "fig, ax, im, cbar = \\\n",
    "    xarray_imshow(max_water_extent, fig=fig, ax=ax, use_colorbar=False, use_legend=True, \n",
    "                  legend_labels=legend_labels, imshow_kwargs=imshow_kwargs,\n",
    "                  x_label_kwargs=axis_label_fmt_dict, y_label_kwargs=axis_label_fmt_dict,\n",
    "                  legend_kwargs=legend_kwargs)\n",
    "ax.set_title(\"Maximum Water Extent \\nTime (YYYY/MM/DD): {}\".format(max_water_date_str), fontdict=title_fontdict)\n",
    "ax.tick_params(**tick_label_fmt_dict)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"water_extent_image\"></a>**Create Water Extent Image** [&#9652;](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:08.312694Z",
     "iopub.status.busy": "2020-09-29T00:57:08.312108Z",
     "iopub.status.idle": "2020-09-29T00:57:09.058521Z",
     "shell.execute_reply": "2020-09-29T00:57:09.058963Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.plotter_utils import binary_class_change_plot\n",
    "\n",
    "fig_width = 6 # Use this to change the size of this figure.\n",
    "fig = plt.figure(figsize=figure_ratio(water_da, fixed_width=fig_width), dpi=std_dpi)\n",
    "\n",
    "# Shade 3 regions - never, sometimes, and always water.\n",
    "(fig,ax), [never_sometimes_always_stats] = \\\n",
    "    binary_class_change_plot(\n",
    "        [water_da==1], [clean_mask], colors=['black', 'yellow', 'cyan'], \n",
    "        class_legend_label='Water', fig=fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:09.072709Z",
     "iopub.status.busy": "2020-09-29T00:57:09.072187Z",
     "iopub.status.idle": "2020-09-29T00:57:09.102732Z",
     "shell.execute_reply": "2020-09-29T00:57:09.102280Z"
    }
   },
   "outputs": [],
   "source": [
    "index = list(map(lambda str: str.format('Water'),\n",
    "                  ['Never {}', 'Sometimes {}', 'Always {}', \n",
    "                   'Max {}', 'Min {}', 'No Data']))\n",
    "\n",
    "num_table_rows = len(index)\n",
    "\n",
    "stats_table_single = pd.DataFrame(data=np.zeros((num_table_rows, 4)),\n",
    "                                  index=index, columns=['Date (YYYY/MM/DD)', 'Number', 'Percent', 'Area (km^2)'])\n",
    "\n",
    "## Date\n",
    "# There are the dates for the rows.\n",
    "dates = np.array(['', '', '', max_water_date_str, min_water_date_str, ''])\n",
    "stats_table_single.loc[:,'Date (YYYY/MM/DD)'] = dates\n",
    "\n",
    "## Number\n",
    "class_sums = np.array([never_sometimes_always_stats.loc['Never Water'].Number, \n",
    "                       never_sometimes_always_stats.loc['Sometimes Water'].Number,\n",
    "                       never_sometimes_always_stats.loc['Always Water'].Number, \n",
    "                       max_water_extent.sum().values, \n",
    "                       min_water_extent.sum().values, \n",
    "                       never_sometimes_always_stats.loc['Unknown'].Number])\n",
    "stats_table_single.loc[:, 'Number'] = class_sums\n",
    "\n",
    "## Area\n",
    "# Show pixel changes and calculate area using pixel resolution\n",
    "prod_info = dc.list_products()\n",
    "deg_per_px = prod_info[prod_info['name'] == products_single[0]]['resolution'].values[0]\n",
    "deg_per_px = np.abs(deg_per_px)\n",
    "# Roughly 111km per degree of latitude and longitude.\n",
    "km_per_px = 111 * deg_per_px\n",
    "# Calculate the total area.\n",
    "sq_km_per_px = np.prod(km_per_px)\n",
    "stats_table_single.loc[:, 'Area (km^2)'] = stats_table_single.loc[:,'Number'] * sq_km_per_px\n",
    "\n",
    "## Percent\n",
    "stats_table_single.loc[:, 'Percent'] = \\\n",
    "    [never_sometimes_always_stats.loc['Never Water'].Percent, \n",
    "     never_sometimes_always_stats.loc['Sometimes Water'].Percent,\n",
    "     never_sometimes_always_stats.loc['Always Water'].Percent, \n",
    "     max_water_extent.mean().values, \n",
    "     min_water_extent.mean().values, \n",
    "     never_sometimes_always_stats.loc['Unknown'].Percent]\n",
    "stats_table_single_fmt = stats_table_single.copy()\n",
    "stats_table_single_fmt.loc[:, 'Percent'] = [f\"{pct:0.2%}\" for pct in stats_table_single.loc[:, 'Percent']]\n",
    "stats_table_single_fmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"time_series_water\"></a>Create a Time Series Plot of the Water [&#9652;](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:09.124413Z",
     "iopub.status.busy": "2020-09-29T00:57:09.122557Z",
     "iopub.status.idle": "2020-09-29T00:57:09.369146Z",
     "shell.execute_reply": "2020-09-29T00:57:09.369619Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.plotter_utils import xarray_time_series_plot\n",
    "from utils.data_cube_utilities.plotter_utils import np_dt64_to_str\n",
    "\n",
    "total_area = len(water_da.latitude) * len(water_da.longitude) * sq_km_per_px\n",
    "\n",
    "figsize = (8, 4) # The width and height of the figure, respectively.\n",
    "fig = plt.figure(figsize=figsize, dpi=std_dpi)\n",
    "\n",
    "(water_da.sum(['latitude', 'longitude']) * sq_km_per_px).plot.line(marker='o')\n",
    "plt.title('Water Area Over Time')\n",
    "plt.ylabel('Area (km^2)')\n",
    "plt.xlabel('Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"two_period\"></a>Compare Two Time Periods - a Baseline and an Analysis [&#9652;](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"define_extents_baseline_analysis\"></a>Define the Extents of the Analysis [&#9652;](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify start and end dates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:09.373817Z",
     "iopub.status.busy": "2020-09-29T00:57:09.373265Z",
     "iopub.status.idle": "2020-09-29T00:57:09.374946Z",
     "shell.execute_reply": "2020-09-29T00:57:09.375353Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select a subset of the time available.\n",
    "time_extents_two = {'baseline': [dt.datetime(2002,1,1), dt.datetime(2002,12,31)],\n",
    "                    'analysis': [dt.datetime(2014,1,1), dt.datetime(2014,12,31)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the selected area**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:09.379463Z",
     "iopub.status.busy": "2020-09-29T00:57:09.378800Z",
     "iopub.status.idle": "2020-09-29T00:57:09.386751Z",
     "shell.execute_reply": "2020-09-29T00:57:09.387169Z"
    }
   },
   "outputs": [],
   "source": [
    "display_map(lat, lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"retrieve_data_baseline_analysis\"></a>Retrieve the Data from the Datacube [&#9652;](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:09.392844Z",
     "iopub.status.busy": "2020-09-29T00:57:09.392428Z",
     "iopub.status.idle": "2020-09-29T00:57:19.549284Z",
     "shell.execute_reply": "2020-09-29T00:57:19.548787Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.dc_mosaic import create_median_mosaic\n",
    "\n",
    "datasets = {} # Maps categories to cleaned datasets.\n",
    "clean_masks = {} # Maps categories to clean masks.\n",
    "datasets_median_composites = {}\n",
    "for category, time_extents in list(time_extents_two.items()):\n",
    "    # Use LANDSAT_7 for the baseline because its SLC was broken afterward (2003).\n",
    "    ind = 0 if category == 'baseline' else 1\n",
    "    platform = platforms_two[ind]\n",
    "    product = products_two[ind]\n",
    "    collection = collections_two[ind]\n",
    "    level = levels_two[ind]\n",
    "    \n",
    "    datasets[category], clean_masks[category] = \\\n",
    "        load_for_time_range([platform], [product], [collection], [level], time_extents)\n",
    "    datasets[category] = datasets[category].persist()\n",
    "    clean_masks[category] = clean_masks[category].persist()\n",
    "    datasets_median_composites[category] = \\\n",
    "        create_median_mosaic(datasets[category], clean_masks[category]).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"water_cls_baseline_analysis\"></a>Get Water Classifications Using the WOFS Algorithm [&#9652;](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:19.853156Z",
     "iopub.status.busy": "2020-09-29T00:57:19.852521Z",
     "iopub.status.idle": "2020-09-29T00:57:19.854328Z",
     "shell.execute_reply": "2020-09-29T00:57:19.854767Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get WOFS classifications.\n",
    "water_das = {}\n",
    "for category in datasets:\n",
    "    water_da = \\\n",
    "        get_water_classifications(datasets[category], clean_masks[category], \n",
    "                                  max_num_filter_runs, \n",
    "                                  denoise, std_denoise_params)\n",
    "    water_das[category] = water_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:20.034774Z",
     "iopub.status.busy": "2020-09-29T00:57:20.019060Z",
     "iopub.status.idle": "2020-09-29T00:57:23.628428Z",
     "shell.execute_reply": "2020-09-29T00:57:23.628797Z"
    }
   },
   "outputs": [],
   "source": [
    "# Resample the data in time to avoid cluttering the x axes of plots \n",
    "# with tick labels (dates) and also smooth the plots.\n",
    "for category in datasets:\n",
    "    datasets[category] = datasets[category].resample(time='2d').mean('time').dropna('time', how='all').persist()\n",
    "    clean_masks[category] = (clean_masks[category].resample(time='2d').mean('time').dropna('time', how='all') >= 0.5).persist()\n",
    "    water_das[category] = water_das[category].resample(time='2d').mean('time').dropna('time', how='all').persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"show_water_extents\"></a>**Show Water Extents of the Baseline and Analysis Periods** [&#9652;](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show the median composite for each period.**\n",
    "<br>The results are displayed using a false color RGB where water is dark blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:23.633984Z",
     "iopub.status.busy": "2020-09-29T00:57:23.633379Z",
     "iopub.status.idle": "2020-09-29T00:57:23.635506Z",
     "shell.execute_reply": "2020-09-29T00:57:23.635066Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.plotter_utils import figure_ratio\n",
    "\n",
    "## Settings ##\n",
    "\n",
    "# Formatting for full-width plots.\n",
    "title_fontdict = dict(fontsize=16) # Title formatting\n",
    "tick_label_fmt_dict = dict(axis='both', labelsize=12) # Tick label formatting\n",
    "axis_label_fmt_dict = dict(fontsize=16) # Axis label formatting\n",
    "legend_kwargs = dict(fontsize=12)\n",
    "std_dpi = 200 # Standard image dpi.\n",
    "\n",
    "def std_figsize(fig_width): \n",
    "    return figure_ratio(datasets['baseline'].isel(time=0), fixed_width=fig_width)\n",
    "\n",
    "# This is the name of the band used as the \n",
    "# graysale background when showing water extents.\n",
    "background_band = 'swir1'\n",
    "\n",
    "## End Settings ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:23.652233Z",
     "iopub.status.busy": "2020-09-29T00:57:23.639575Z",
     "iopub.status.idle": "2020-09-29T00:57:24.300620Z",
     "shell.execute_reply": "2020-09-29T00:57:24.301062Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.dc_rgb import rgb\n",
    "\n",
    "fig_width = 12 # Use this to change the size of this figure.\n",
    "fig, ax = plt.subplots(1, 2, figsize=std_figsize(fig_width))\n",
    "\n",
    "for i, category in enumerate(water_das):\n",
    "    rgb(datasets_median_composites[category], fig=fig, ax=ax[i], \n",
    "        bands=['swir2', 'nir', 'green'], imshow_kwargs=dict(vmin=0, vmax=3000))\n",
    "    ax[i].set_title(category.capitalize())\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show which pixels are sometime water or always water for each period, with a gray-scale background.**\n",
    "<br>Warning: These results can be impacted by cloud contamination. The identification of clouds in Landsat <br>scenes is not perfect, so errors in cloud identification can impact time series results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:24.322330Z",
     "iopub.status.busy": "2020-09-29T00:57:24.320389Z",
     "iopub.status.idle": "2020-09-29T00:57:25.004977Z",
     "shell.execute_reply": "2020-09-29T00:57:25.005417Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.plotter_utils import binary_class_change_plot\n",
    "\n",
    "fig_width = 12 # Use this to change the size of this figure.\n",
    "fig, ax = plt.subplots(1, 2, figsize=std_figsize(fig_width))\n",
    "\n",
    "# Collect stats tables for each period so we can later create a change matrix.\n",
    "stats_tables = {}\n",
    "\n",
    "for i, category in enumerate(water_das):\n",
    "    # Show water in blue over a grey-scale background.\n",
    "    rgb(datasets_median_composites[category], bands=[background_band]*3, \n",
    "        ax=ax[i])\n",
    "    water_composite = water_das[category].where(clean_masks[category], 0).mean('time')\n",
    "    [fig, ax[i]], [stats_tables[category]] = \\\n",
    "    binary_class_change_plot(\n",
    "        [water_das[category]==1],\n",
    "        [clean_masks[category]],\n",
    "        colors=['gray', 'yellow', 'blue'], \n",
    "        neg_trans=True,\n",
    "        class_legend_label='Water', fig=fig, ax=ax[i], \n",
    "        title_kwargs=dict(label='Water ' + category.capitalize(), fontdict=title_fontdict),\n",
    "        x_label_kwargs=axis_label_fmt_dict, y_label_kwargs=axis_label_fmt_dict, \n",
    "        legend_kwargs=legend_kwargs, \n",
    "        denoise=denoise, denoise_params=std_denoise_params)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"show_analyze_diff\"></a>**Show and Analyze the Differences Between the Two Time Periods** [&#9652;](#top)\n",
    "\n",
    "Compare the water extents of the cloud-filtered mosaics of the baseline and analysis time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:25.030023Z",
     "iopub.status.busy": "2020-09-29T00:57:25.023725Z",
     "iopub.status.idle": "2020-09-29T00:57:25.502581Z",
     "shell.execute_reply": "2020-09-29T00:57:25.503021Z"
    }
   },
   "outputs": [],
   "source": [
    "fig_width = 10 # Use this to change the size of this figure.\n",
    "fig, ax = plt.subplots(figsize=std_figsize(fig_width))\n",
    "\n",
    "# Show water change over a grey-scale background.\n",
    "rgb(datasets_median_composites['baseline'], bands=[background_band]*3, \n",
    "    ax=ax)\n",
    "# Show regions based on changes between having zero or more than zero times \n",
    "# in which they are classified as water between the two time periods.\n",
    "[fig, ax], [stats_table_two, change_matrix] = \\\n",
    "    binary_class_change_plot(\n",
    "        [water_das['baseline']==1, water_das['analysis']==1],\n",
    "        [clean_masks['baseline'], clean_masks['analysis']],\n",
    "        colors=['gray', 'green', 'red', 'blue'], \n",
    "        neg_trans=True,\n",
    "        class_legend_label='Water', fig=fig, ax=ax, title_kwargs=dict(fontdict=title_fontdict),\n",
    "        x_label_kwargs=axis_label_fmt_dict, y_label_kwargs=axis_label_fmt_dict, \n",
    "        legend_kwargs=legend_kwargs, \n",
    "        denoise=denoise, denoise_params=std_denoise_params)\n",
    "ax.tick_params(**tick_label_fmt_dict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:25.509454Z",
     "iopub.status.busy": "2020-09-29T00:57:25.508772Z",
     "iopub.status.idle": "2020-09-29T00:57:25.525457Z",
     "shell.execute_reply": "2020-09-29T00:57:25.525901Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add area to the stats table.\n",
    "# 1. Get resolution. In this case, we know the x and y coordinates are longitude and latitude\n",
    "#    and that the products all have the same resolution.\n",
    "prod_info = dc.list_products()\n",
    "deg_per_px = prod_info[prod_info['name'] == products_two[0]]['resolution'].values[0]\n",
    "deg_per_px = np.abs(deg_per_px)\n",
    "# Roughly 111km per degree of latitude and longitude.\n",
    "km_per_px = 111 * deg_per_px\n",
    "# 2. Calculate the total area\n",
    "length_in_km = km_per_px * np.array([len(water_das['baseline'].latitude),\n",
    "                                         len(water_das['baseline'].longitude)])\n",
    "total_area_in_km = np.prod(length_in_km)\n",
    "# 3. Compute the area for each row of the stats table and display the table.\n",
    "stats_table_two.loc[:, 'Area (km^2)'] = stats_table_two.loc[:,'Percent'] * total_area_in_km\n",
    "\n",
    "# Format percent as strings.\n",
    "stats_table_two_fmt = stats_table_two.copy()\n",
    "stats_table_two_fmt.loc[:, 'Percent'] = [f\"{pct:0.2%}\" for pct in stats_table_two.loc[:, 'Percent']]\n",
    "stats_table_two_fmt.loc[:, 'Area (km^2)'] = [f\"{area:.3f}\" for area in stats_table_two.loc[:, 'Area (km^2)']]\n",
    "stats_table_two_fmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:25.540432Z",
     "iopub.status.busy": "2020-09-29T00:57:25.539987Z",
     "iopub.status.idle": "2020-09-29T00:57:25.826444Z",
     "shell.execute_reply": "2020-09-29T00:57:25.827101Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.dc_utilities import reverse_array_dict\n",
    "from utils.data_cube_utilities.plotter_utils import create_discrete_color_map\n",
    "from utils.data_cube_utilities.plotter_utils import print_matrix\n",
    "\n",
    "# Create a colormap for coloring the cells in the change matrix.\n",
    "# The colors are, in order, gray, red, green, yellow, and blue. Many are manually specified\n",
    "# as rgb lists because their matplotlib named colors are fairly bright and saturated.\n",
    "change_matrix_cmap = \\\n",
    "    create_discrete_color_map([0,4], colors=['gray', [225,16,16], [16,128,16], [235,235,0], [32,32,192]])\n",
    "\n",
    "# This is a matrix specifying each class transition based on the color it should recieve.\n",
    "# Original classes are indexed by row and final classes are indexed by column.\n",
    "cng_vals = [0, 1, 2, 3, 4]\n",
    "cng_mat = np.array([\n",
    "    # Always Sometimes Never\n",
    "    [     4,      1,      1], # Always\n",
    "    [     2,      3,      1], # Sometimes \n",
    "    [     2,      2,      0], # Never   \n",
    "])\n",
    "\n",
    "# Create mappings of transitions to values for coloring based on a colormap.\n",
    "cls_trans_for_value = {cng_val:[] for cng_val in cng_vals}\n",
    "for i, orig_class in enumerate(change_matrix.baseline.values):\n",
    "    for j, final_class in enumerate(change_matrix.analysis.values):\n",
    "        cng_val = cng_mat[i,j]\n",
    "        cls_trans_for_value[cng_val].append((orig_class, final_class))\n",
    "value_for_cls_trans = reverse_array_dict(cls_trans_for_value)\n",
    "\n",
    "# Create the cell value matrix (used to color cells).\n",
    "cell_value_mtx = np.empty_like(change_matrix.Percent.values)\n",
    "for i, cls_label1 in enumerate(change_matrix.baseline.values):\n",
    "    for j, cls_label2 in enumerate(change_matrix.analysis.values):\n",
    "        cell_value_mtx[i,j] = value_for_cls_trans[(cls_label1, cls_label2)]\n",
    "\n",
    "cell_label_mtx = np.full_like(cell_value_mtx, '', dtype=object)\n",
    "for i, baseline_category in enumerate(change_matrix['baseline']):\n",
    "    for j, analysis_category in enumerate(change_matrix['analysis']):\n",
    "        cell_label_mtx[i,j] = \"{0:.2%}\".format(change_matrix.Percent.values[i,j])\n",
    "\n",
    "row_labels = [label.capitalize() for label in change_matrix.baseline.values]\n",
    "col_labels = [label.capitalize() for label in change_matrix.analysis.values]\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "fig, ax = print_matrix(cell_value_mtx, cell_val_fmt='s',\n",
    "                       cell_label_mtx=cell_label_mtx, cmap=change_matrix_cmap,\n",
    "                       row_labels=row_labels, col_labels=col_labels, \n",
    "                       x_axis_ticks_position='top', x_axis_tick_kwargs={},\n",
    "                       annot_kwargs=dict(size=14), fig=fig)\n",
    "ax.yaxis.set_label_position('left')\n",
    "plt.ylabel('Baseline Class', fontsize=18)\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.xlabel('Analysis Class', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the area of each of the 9 transition classes in the change matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:25.837574Z",
     "iopub.status.busy": "2020-09-29T00:57:25.836769Z",
     "iopub.status.idle": "2020-09-29T00:57:25.881237Z",
     "shell.execute_reply": "2020-09-29T00:57:25.880782Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Get resolution. In this case, we know the x and y coordinates are longitude and latitude\n",
    "#    and that the products all have the same resolution.\n",
    "prod_info = dc.list_products()\n",
    "deg_per_px = prod_info[prod_info['name'] == products_two[0]]['resolution'].values[0]\n",
    "deg_per_px = np.abs(deg_per_px)\n",
    "# Roughly 111km per degree of latitude and longitude.\n",
    "meters_per_px = 111000 * deg_per_px\n",
    "\n",
    "change_matrix['Area (m^2)'] = change_matrix.Number * np.prod(meters_per_px)\n",
    "cng_mtx_as_table = change_matrix.to_dataframe().reset_index()\n",
    "baseline_analysis_cls_strs = cng_mtx_as_table[['baseline', 'analysis']].values\n",
    "cng_mtx_as_table.drop(columns=['analysis', 'baseline'], inplace=True)\n",
    "cng_mtx_as_table.index = [\"{} to {}\".format(base_cls, analysis_cls) for base_cls, analysis_cls \n",
    "                            in baseline_analysis_cls_strs]\n",
    "cng_mtx_as_table.style.format({'percent': \"{:.2%}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"classify_by_degree_change\"></a>**Classify the Area by Degree of Change** [&#9652;](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-29T00:57:25.890201Z",
     "iopub.status.busy": "2020-09-29T00:57:25.889646Z",
     "iopub.status.idle": "2020-09-29T00:57:25.891848Z",
     "shell.execute_reply": "2020-09-29T00:57:25.892259Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the net change of water from the baseline to the analysis time periods.\n",
    "starting_water_num = stats_table_two.loc[['Water to No Water', 'Water to Water'], ['Number']].sum()[0]\n",
    "net_change_water_num = (stats_table_two.loc['No Water to Water', 'Number'] - \\\n",
    "                        stats_table_two.loc['Water to No Water', 'Number'])\n",
    "net_change_water_pct = net_change_water_num / starting_water_num\n",
    "print(\"The percent of change in the water extent from the baseline time period \" \\\n",
    "      \"to the analysis time period (relative change from the baseline) \"\\\n",
    "      \"is {0:.2%}.\\n\".format(net_change_water_pct))\n",
    "\n",
    "# Classify the area by degree of change.\n",
    "# Transitions from never water to some/always water (and vice versa) are considered changes.\n",
    "frac_change_cls = stats_table_two.loc[['No Water to Water', 'Water to No Water'], ['Percent']].sum()[0]\n",
    "\n",
    "region_class = None\n",
    "frac_change_cls = abs(frac_change_cls)\n",
    "if frac_change_cls <= 0.1:\n",
    "    region_class = \"Unmodified Natural (class A)\"\n",
    "if 0.1 < frac_change_cls <= 0.2:\n",
    "    region_class = \"Largely Natural (class B)\"\n",
    "if 0.2 < frac_change_cls <= 0.4:\n",
    "    region_class = \"Moderately Modified (class C)\"\n",
    "if 0.4 < frac_change_cls <= 0.6:\n",
    "    region_class = \"Largely Modified (class D)\"\n",
    "if 0.6 < frac_change_cls:\n",
    "    region_class = \"Seriously Modified (class E)\"\n",
    "print(\"The percent of pixels that are different water classes between the baseline and analysis time periods \" \\\n",
    "      \"is {0:.2%}, so this region can be classified as {1}\".format(frac_change_cls, region_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
