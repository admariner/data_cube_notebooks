{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive waterbody relative topography using Landsat\n",
    "\n",
    "This notebook demonstrates how to load Landsat time series data, compute a water index, generate a rolling median water index composites, extract contours along the land-water boundary, and finally interpolate between contours to produce a 3D relative topographic surface. This relative topography could be easily calibrated to obtain absolute bathymetry (and accordingly, volume estimates) with a simple GPS transect from the highest to the deepest part of the lake during a dry period.\n",
    "\n",
    "**Original Author**: Robbi Bishop-Taylor\n",
    "\n",
    "**Original Date**: 30 October 2018\n",
    "\n",
    "**Original Notebook**: https://github.com/digitalearthafrica/deafrica-sandbox-notebooks/blob/master/RCMRD_Demo/colombo_workshop/GA_Water_3DReservoir.ipynb\n",
    "\n",
    "**Chunking Modifications Author**: John Rattz\n",
    "\n",
    "**Chunking Modification Date**: 4 October 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.environ.get('NOTEBOOK_ROOT'))\n",
    "\n",
    "# Supress Warning \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datacube\n",
    "import glob\n",
    "import rasterio\n",
    "import scipy\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from skimage import filters\n",
    "from skimage import exposure\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterstats import zonal_stats\n",
    "import time\n",
    "\n",
    "from utils.data_cube_utilities.clean_mask import \\\n",
    "    landsat_qa_clean_mask, landsat_clean_mask_invalid\n",
    "from utils.data_cube_utilities.dc_display_map import display_map\n",
    "from utils.data_cube_utilities.import_export import export_xarray_to_netcdf\n",
    "\n",
    "from ga_utils import contour_extract\n",
    "from ga_utils import contours_to_arrays\n",
    "from ga_utils import interpolate_timeseries\n",
    "\n",
    "from datacube.utils.aws import configure_s3_access\n",
    "configure_s3_access(requester_pays=True)\n",
    "\n",
    "dc = datacube.Datacube(app = 'my_app')\n",
    "\n",
    "import os\n",
    "sub_dir = 'example'\n",
    "output_dir = f'output/{sub_dir}'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "no_data = -9999\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lake Sulunga\n",
    "# platform = \"LANDSAT_8\"\n",
    "# product = \"ls8_usgs_sr_scene\" \n",
    "# collection = 'c1'\n",
    "# level = 'l2'\n",
    "# lat = (-5.86, -6.27)\n",
    "# lon = (34.97, 35.38)\n",
    "# time_extents = ('2018-01-01', '2018-12-31')\n",
    "\n",
    "# Lake Balangida\n",
    "# platform = \"LANDSAT_8\"\n",
    "# product = \"ls8_usgs_sr_scene\" \n",
    "# collection = 'c1'\n",
    "# level = 'l2'\n",
    "# lat = (-4.60, -4.76)\n",
    "# lon = (35.135, 35.295)\n",
    "# time_extents = ('2013-01-01', '2018-11-01')\n",
    "\n",
    "# Lake Chala (small)\n",
    "platform = \"LANDSAT_8\"\n",
    "product = \"ls8_usgs_sr_scene\"\n",
    "collection = 'c1'\n",
    "level = 'l2'\n",
    "lat = (-3.3282, -3.3065)\n",
    "lon = (37.6871, 37.7140)\n",
    "time_extents = ('2014-01-01', '2014-06-30')\n",
    "\n",
    "# Lake Nakuru\n",
    "# platform = \"LANDSAT_8\"\n",
    "# product = \"ls8_usgs_sr_scene\"\n",
    "# collection = 'c1'\n",
    "# level = 'l2'\n",
    "# lat = (-0.30, -0.42) \n",
    "# lon = (36.05, 36.13)\n",
    "# time_extents = ('2013-01-01', '2018-11-01')\n",
    "\n",
    "#  Lake Volta, Ghana\n",
    "# platform = \"LANDSAT_8\"\n",
    "# product = \"ls8_usgs_sr_scene\" \n",
    "# collection = 'c1'\n",
    "# level = 'l2'\n",
    "# Full\n",
    "# lat = (6.1914, 8.9334) \n",
    "# lon = (-1.4526, 0.8276)\n",
    "# time_extents = ('2013-01-01', '2018-12-31')\n",
    "# small subset in Eastern Region\n",
    "# lat = (6.7219, 6.8092) \n",
    "# lon = (-0.6406, -0.5033)\n",
    "# time_extents = ('2016-01-01', '2018-12-31')\n",
    "\n",
    "# Lake Naivasha\n",
    "# platform = \"LANDSAT_8\"\n",
    "# product = \"ls8_usgs_sr_scene\" \n",
    "# collection = 'c1'\n",
    "# level = 'l2'\n",
    "# lat = (-0.8350, -0.6700) \n",
    "# lon = (36.2600, 36.4300)\n",
    "# time_extents = ('2013-01-01', '2013-12-31')\n",
    "\n",
    "# Lake in Singida, Tanzania\n",
    "# platform = \"LANDSAT_8\"\n",
    "# product = \"ls8_usgs_sr_scene\" \n",
    "# collection = 'c1'\n",
    "# level = 'l2'\n",
    "# lat = (-4.665, -4.751)\n",
    "# lon = (34.80, 34.885)\n",
    "# time_extents = ('2013-01-01', '2018-11-01')\n",
    "\n",
    "# Marigot de Bignone\n",
    "# platform = \"LANDSAT_8\"\n",
    "# product = \"ls8_lasrc_senegal\" \n",
    "# collection = 'c1'\n",
    "# level = 'l2'\n",
    "# lon = (-16.43, -16.21)\n",
    "# lat = (12.83, 12.65)\n",
    "# time_extents = ('2013-01-01', '2018-11-01')\n",
    "\n",
    "display_map(lat, lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain data for outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings ##\n",
    "\n",
    "# Contour extraction and interpolation parameters\n",
    "min_vertices = 5  # This can be used to remove noise by dropping contours with less than X vertices\n",
    "guassian_sigma = 0  # Controls amount of smoothing to apply to interpolated raster. Higher = smoother\n",
    "\n",
    "# The water index to use as a proxy of water extent.\n",
    "water_index = 'mndwi' # Can be any of ['mndwi', 'ndwi', 'awei']\n",
    "\n",
    "## End Settings ##\n",
    "\n",
    "water_index_req_bands = {'mndwi': ['green', 'swir1'],\n",
    "                         'ndwi': ['green', 'nir'],\n",
    "                         'awei': ['green', 'swir1','nir','swir2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = list(set(water_index_req_bands[water_index] + ['pixel_qa'] +\\\n",
    "               ['red', 'green', 'blue']))\n",
    "data = dc.load(latitude = lat,\n",
    "               longitude = lon,\n",
    "               platform = platform,\n",
    "               time = time_extents,\n",
    "               product = product,\n",
    "               measurements = measurements,\n",
    "               group_by='solar_day',\n",
    "               dask_chunks={'time':5, 'latitude':1000, 'longitude':1000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get max water extent mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.clean_mask import landsat_clean_mask_full\n",
    "\n",
    "## Clean the data. ##\n",
    "clean_mask = landsat_clean_mask_full(dc, data, product=product, platform=platform,\n",
    "                                     collection=collection, level=level)\n",
    "cleaned_data = data.where(clean_mask)\n",
    "\n",
    "## Compute water index. ##\n",
    "if water_index == 'mndwi':\n",
    "    cleaned_data[water_index] = (cleaned_data.green - cleaned_data.swir1) / \\\n",
    "                                 (cleaned_data.green + cleaned_data.swir1)\n",
    "elif water_index == 'ndwi':\n",
    "    cleaned_data[water_index] = (cleaned_data.green - cleaned_data.nir) / \\\n",
    "                                 (cleaned_data.green + cleaned_data.nir)\n",
    "else: # AWEI\n",
    "    cleaned_data[water_index] = 4 * (cleaned_data.green * 0.0001 - cleaned_data.swir1 * 0.0001) - \\\n",
    "                                 (0.25 * cleaned_data.nir * 0.0001 + 2.75 * cleaned_data.swir2 * 0.0001)\n",
    "\n",
    "## Obtain the max water mask. ##\n",
    "max_water_mask = (cleaned_data[water_index].fillna(-1) > 0).max('time').persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data and inundation percents for each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute percentages of valid data and inundation. ##\n",
    "# Create mask of max extent of water (land = 0, water = 1) and set all pixels \n",
    "# outside max extent area to NaN.\n",
    "water_masked = cleaned_data[water_index].where(max_water_mask)\n",
    "\n",
    "# Calculate the valid data percentage for each time step by dividing the number of \n",
    "# non-NaN pixels in timestep by the total number of pixels in the max extent water layer\n",
    "data_perc = water_masked.count(['latitude', 'longitude']) /\\\n",
    "            max_water_mask.sum()\n",
    "cleaned_data['data_perc'] = data_perc\n",
    "\n",
    "## Calculate inundation percent. ##\n",
    "inundation_perc = (water_masked > 0).sum(['latitude', 'longitude']) \\\n",
    "                  / max_water_mask.sum()\n",
    "cleaned_data['inundation_perc'] = inundation_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to scenes with greater than 20% valid data and select variables for further analysis\n",
    "cleaned_data = cleaned_data.sortby('inundation_perc', ascending=False)\n",
    "times_to_keep = cleaned_data.data_perc > 0.2\n",
    "cleaned_subset = cleaned_data.sel(time=times_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain rolling median water composites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the minimum and maximum time indices for the median water composites.\n",
    "min_max_indices_comp = []\n",
    "for i, time_ind in enumerate(np.arange(0, len(cleaned_subset.time), 5)):\n",
    "    # identify min and max index to extract rolling median\n",
    "    min_index = max(time_ind - 15, 0)\n",
    "    max_index = min(time_ind + 15, len(cleaned_subset.time)-1)\n",
    "    min_max_indices_comp.append((min_index, max_index))\n",
    "num_rolling_composites = len(min_max_indices_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_water_composites = []\n",
    "time_range_strs = [] # Used to label output files.\n",
    "\n",
    "for comp_ind, time_inds in enumerate(min_max_indices_comp):\n",
    "    time_range_strs.append(\n",
    "        '_'.join([np.datetime_as_string(t, unit='D') for t in \n",
    "                  cleaned_subset.time.values[list(time_inds)]]))\n",
    "    \n",
    "    rolling_water_composite = cleaned_subset[[water_index, 'inundation_perc']].isel(time=slice(*time_inds)).mean('time')\n",
    "    \n",
    "    rolling_water_composites.append(rolling_water_composite)\n",
    "\n",
    "combined = xr.concat(rolling_water_composites, dim='time_period').sortby('time_period')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot only observations with greater than 20% valid data\n",
    "timeseries_subset = cleaned_data.inundation_perc.sel(time = times_to_keep)\n",
    "\n",
    "# # Interpolate to one point per week, then take a rolling mean to smooth line for plotting\n",
    "timeseries_subset = interpolate_timeseries(timeseries_subset.sortby('time').chunk({'time': -1}), \n",
    "                                           freq='3D', method='linear')\n",
    "timeseries_subset = timeseries_subset.rolling(time=5, min_periods=1).mean()\n",
    "timeseries_subset.plot(size=5)\n",
    "\n",
    "# Export to text file\n",
    "name = 'inundation_perc'\n",
    "timeseries_subset_df = timeseries_subset.to_dataframe(name=name)\n",
    "timeseries_subset_df['date'] = timeseries_subset_df.index.floor('d')\n",
    "timeseries_subset_df.set_index('date')\n",
    "timeseries_subset_df.to_csv(output_dir + '/{}_timeseries.csv'.format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the contours and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "observations = combined.inundation_perc\n",
    "\n",
    "for i, observation in enumerate(observations):    \n",
    "    output_shp = f\"{output_dir}/{name}_{time_range_strs[i]}.shp\"\n",
    "    if os.path.exists(output_shp):\n",
    "        continue\n",
    "    \n",
    "    cleaned_subset_i = combined.isel(time_period=i)\n",
    "    \n",
    "    # Compute area\n",
    "    area = float(cleaned_subset_i.inundation_perc.values) * 100 \n",
    "    \n",
    "    # Prepare attributes as input to contour extract\n",
    "    attribute_data = {'in_perc': [area]}\n",
    "    attribute_dtypes = {'in_perc': 'float'}\n",
    "    \n",
    "    # Set threshold\n",
    "    thresh = 0 \n",
    "    \n",
    "    # Extract contours with custom attribute fields:\n",
    "    contour_dict = contour_extract(z_values=[thresh],\n",
    "                                   ds_array=cleaned_subset_i[water_index].values,\n",
    "                                   ds_crs='epsg:4326',\n",
    "                                   ds_affine=data.geobox.transform,\n",
    "                                   output_shp=output_shp,\n",
    "                                   min_vertices=min_vertices,  \n",
    "                                   attribute_data=attribute_data,\n",
    "                                   attribute_dtypes=attribute_dtypes)\n",
    "        \n",
    "# Combine all shapefiles into one file\n",
    "shapefiles = glob.glob(output_dir + '/{}_*.shp'.format(name))\n",
    "gdf = pd.concat([gpd.read_file(shp) for shp in shapefiles], sort=False).pipe(gpd.GeoDataFrame)\n",
    "\n",
    "# Set CRS\n",
    "gdf['crs'] = 'EPSG:4326'\n",
    "\n",
    "# Plot contours\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "gdf.plot(ax=ax, column='in_perc', cmap='viridis', linewidth=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate DEM values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x, y and z points for interpolation\n",
    "all_contours = contours_to_arrays(gdf=gdf, col='in_perc')\n",
    "points_xy = all_contours[:, [1, 0]]\n",
    "values_elev = all_contours[:, 2]\n",
    "\n",
    "# Create grid to interpolate into\n",
    "x_size, _, upleft_x, _, y_size, upleft_y = data.geobox.transform[0:6]\n",
    "xcols = len(data.longitude)\n",
    "yrows = len(data.latitude)\n",
    "bottomright_x = upleft_x + (x_size * xcols)\n",
    "bottomright_y = upleft_y + (y_size * yrows)\n",
    "grid_y, grid_x = np.mgrid[upleft_y:bottomright_y:1j * yrows, upleft_x:bottomright_x:1j * xcols]\n",
    "\n",
    "# Interpolate x, y and z values using linear/TIN interpolation\n",
    "out = scipy.interpolate.griddata(points_xy, values_elev, (grid_y, grid_x), method='linear')\n",
    "\n",
    "# Set areas outside of water composite to highest inundation percentage\n",
    "test = (combined[water_index] > 0).max(dim='time_period')\n",
    "out[~test] = np.nanmax(out)\n",
    "out[np.isnan(out)] = np.nanmax(out)\n",
    "\n",
    "# Apply guassian blur to smooth transitions between z values (optional)\n",
    "out = filters.gaussian(out, sigma=guassian_sigma)\n",
    "out = exposure.rescale_intensity(out, out_range=(timeseries_subset.min().values - 0.001, \n",
    "                                                 timeseries_subset.max().values + 0.001))\n",
    "\n",
    "# Plot interpolated surface\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "ax.imshow(out, cmap='magma_r', extent=[upleft_x, bottomright_x, bottomright_y, upleft_y])\n",
    "gdf.plot(ax=ax, edgecolor='white', linewidth=0.5, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export DEM and RGB arrays to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'driver': 'GTiff',\n",
    "         'width': xcols,\n",
    "         'height': yrows,\n",
    "         'count': 1,\n",
    "         'dtype': rasterio.float64,\n",
    "         'crs': 'EPSG:4326',\n",
    "         'transform': data.geobox.transform,\n",
    "         'nodata': no_data}\n",
    "\n",
    "with rasterio.open(output_dir + '/{}_dem.tif'.format(name), 'w', **kwargs) as target:\n",
    "    target.write_band(1, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that only one of the options below is uncommented.\n",
    "\n",
    "# Option 1: \n",
    "#     Select nearly cloud-free images with low inundation. \n",
    "#     You may need to tune the thresholds for `data_perc` and\n",
    "#     `inundation_perc`. There may be no suitable data if\n",
    "#     (1) the water body does not recede much during the selected time, or\n",
    "#     (2) there is too much cloud cover for the selected time.\n",
    "# rgb_times = time_coords[(data.data_perc > 0.9) & (data.inundation_perc < 0.6)]\n",
    "# rgb_times = cleaned_data.time.values\\\n",
    "#     [(cleaned_data.data_perc > 0.9).values & \\\n",
    "#      (cleaned_data.inundation_perc < 0.6).values]\n",
    "\n",
    "# Option 2 (if Option 1 is untenable):\n",
    "#     Get rgb values from a composite of all the data.\n",
    "rgb_times = cleaned_data.time.values[[0,-1]]\n",
    "\n",
    "# Obtain a mean composite of the RGB values.\n",
    "rgb_composite = \\\n",
    "    cleaned_data[['red', 'green', 'blue']]\\\n",
    "    .sel(time = rgb_times)\\\n",
    "    .mean('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = rgb_composite.to_array().values\n",
    "\n",
    "# Optimise colours using a percentile stretch\n",
    "rgb_array = np.transpose(data_array, [1, 2, 0])\n",
    "p_low, p_high = np.nanpercentile(rgb_array, [2, 98])\n",
    "img_toshow = exposure.rescale_intensity(rgb_array, in_range=(p_low, p_high), out_range=(0, 1))\n",
    "\n",
    "# Change dtype to int16 scaled between 0 and 10000 to save disk space\n",
    "img_toshow = (img_toshow * 10000).astype(rasterio.int16)\n",
    "\n",
    "kwargs = {'driver': 'GTiff',\n",
    "         'width': xcols,\n",
    "         'height': yrows,\n",
    "         'count': 3,\n",
    "         'dtype': rasterio.int16,\n",
    "         'crs': 'EPSG:4326',\n",
    "         'transform': data.geobox.transform,\n",
    "         'nodata': no_data}\n",
    "\n",
    "with rasterio.open(output_dir + '/{}_rgb.tif'.format(name), 'w', **kwargs) as target:\n",
    "    target.write(np.transpose(img_toshow, [2, 0, 1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
